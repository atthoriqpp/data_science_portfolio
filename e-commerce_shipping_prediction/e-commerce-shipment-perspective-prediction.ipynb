{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# E-Commerce Shipment Perspective & Prediction\n# Project Background\nCurrently, our e-commerce On-Time Delivery Rate in this dataset is only 40%.\n\nIn the world of e-commerce, on-time delivery is essential for customer satisfaction, where 32%\nof customers complain about late delivery, according to the results of a survey titled [“Biggest\nDrawbacks of E-Commerce Purchases (Worldwide, 2022)” by Koen van Gelder via Statista](https://www.statista.com/statistics/1308184/online-shopping-drawbacks-worldwide/).\nThe survey also confirms that late delivery is **one of the four major problems** faced by\ne-commerce users.\n\nWe will focus on the **On-Time Delivery Rate** as the main business metric.\n\nPeople involved in this project:\n1. [Atthoriq Putra Pangestu (Team Lead)](https://www.linkedin.com/in/atthoriqputra/)\n2. [Alvida Dwiki Chairunnisa](https://www.linkedin.com/in/alvidadwikic/)\n3. [Darell Timothy Tarigan](https://www.linkedin.com/in/darell-timothy-tarigan-533238247/)\n4. [Emir Akbar](https://www.linkedin.com/in/emir-akbar-7b69b0260/)\n5. [Nur Baiti Listyaningrum](https://www.linkedin.com/in/nur-baiti-listyaningrum/)\n6. [Reny Rafiqah](https://www.linkedin.com/in/reny-rafiqah-41b050103/)","metadata":{}},{"cell_type":"markdown","source":"# Environment and Dataset Preparation\nBefore we begin exploring the dataset, let's prepare the dataset and libraries required for this project.","metadata":{"id":"xZHyF-J2d6N5"}},{"cell_type":"code","source":"# Install the SHAP package\n!pip install shap","metadata":{"id":"Fj9myIyPldrU","outputId":"f47a274e-7db2-4ca1-d7ac-3191eb749535","execution":{"iopub.status.busy":"2023-11-11T02:44:42.592601Z","iopub.execute_input":"2023-11-11T02:44:42.593048Z","iopub.status.idle":"2023-11-11T02:44:58.990013Z","shell.execute_reply.started":"2023-11-11T02:44:42.592996Z","shell.execute_reply":"2023-11-11T02:44:58.988453Z"},"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Data manipulation library\nimport numpy as np\nimport pandas as pd\n\n# Data visualization library\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Statistics\nimport scipy.stats as st\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\n# Modeling\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import PowerTransformer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import PowerTransformer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\nfrom sklearn.model_selection import cross_validate\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.svm import SVC\n\nfrom sklearn.model_selection import RandomizedSearchCV\n\nimport shap\n\n# Hide warnings\nimport warnings \nwarnings.filterwarnings(\"ignore\")","metadata":{"id":"cYlmVAoveD_Q","execution":{"iopub.status.busy":"2023-11-11T02:44:58.992538Z","iopub.execute_input":"2023-11-11T02:44:58.994048Z","iopub.status.idle":"2023-11-11T02:45:07.435288Z","shell.execute_reply.started":"2023-11-11T02:44:58.993980Z","shell.execute_reply":"2023-11-11T02:45:07.433742Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/customer-analytics/Train.csv')","metadata":{"id":"5Gu76EpVe3LG","execution":{"iopub.status.busy":"2023-11-11T02:45:07.437136Z","iopub.execute_input":"2023-11-11T02:45:07.437746Z","iopub.status.idle":"2023-11-11T02:45:07.496375Z","shell.execute_reply.started":"2023-11-11T02:45:07.437709Z","shell.execute_reply":"2023-11-11T02:45:07.495305Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EDA\n\n## Descriptive Analysis\nLet's first quickly inspect the data types that each of our feature has.","metadata":{"id":"Z_2L4qImZ1TX"}},{"cell_type":"code","source":"df.info()","metadata":{"id":"4E40GFEE3UM0","outputId":"5d87c46f-2321-4e04-f1aa-a4b475e25627","execution":{"iopub.status.busy":"2023-11-11T02:45:07.499456Z","iopub.execute_input":"2023-11-11T02:45:07.500257Z","iopub.status.idle":"2023-11-11T02:45:07.542104Z","shell.execute_reply.started":"2023-11-11T02:45:07.500210Z","shell.execute_reply":"2023-11-11T02:45:07.540829Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The following changes will be made for visualization purposes:\n\n- `ID` is more suitable in a different data format: object.\n- `Reached.on.Time_Y.N` (target) is more suitable in a different data format: object (because the model used is classification).\n- `Customer_rating` should be in a different data format: object.\n- The target `Reached.on.Time_Y.N` needs to be renamed to `Delivery_status` to make it easier to use:\n    - 0 --> On time\n    - 1 --> Late\n- The other columns seem appropriate.","metadata":{"id":"kwu2aZkhBiEO"}},{"cell_type":"code","source":"# Changing ID to an object\ndf['ID'] = df['ID'].astype('str')\n\n# Changing Customer_rating to object\ndf['Customer_rating'] = df['Customer_rating'].astype('str')\n\n# Changing target variable Reached.on.Time_Y.N to object\ndf['Reached.on.Time_Y.N'] = df['Reached.on.Time_Y.N'].astype('str')\n\n# Renaming target name\ndf.rename(columns = {'Reached.on.Time_Y.N' : 'Delivery_status'}, inplace = True)\n\n# Showing the changes\ndf.info()","metadata":{"id":"hW8nbtrKc7Ii","outputId":"1a27d56d-465a-4384-d9f9-44d9027e5813","execution":{"iopub.status.busy":"2023-11-11T02:45:07.543683Z","iopub.execute_input":"2023-11-11T02:45:07.544116Z","iopub.status.idle":"2023-11-11T02:45:07.609444Z","shell.execute_reply.started":"2023-11-11T02:45:07.544081Z","shell.execute_reply":"2023-11-11T02:45:07.608210Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After that, we can check the number of missing values that each feature has.","metadata":{}},{"cell_type":"code","source":"# Double check the missing values\ndf.isnull().sum()","metadata":{"id":"85yeQeQZ1u3H","outputId":"eac2d049-f87a-4466-f3a7-2b1c51fbdd2b","execution":{"iopub.status.busy":"2023-11-11T02:45:07.611678Z","iopub.execute_input":"2023-11-11T02:45:07.612189Z","iopub.status.idle":"2023-11-11T02:45:07.636297Z","shell.execute_reply.started":"2023-11-11T02:45:07.612141Z","shell.execute_reply":"2023-11-11T02:45:07.634932Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Observation result:\n- There are no missing values across all columns.\n\nWhat about duplicates?","metadata":{"id":"3nQNpQRyN5ya"}},{"cell_type":"code","source":"# Checking for duplicates\nprint(f'Duplicate observations: {df.duplicated().sum()}')","metadata":{"id":"XNeNXomYAOBA","outputId":"290d2f57-862a-406b-b4db-e966c44e11ca","execution":{"iopub.status.busy":"2023-11-11T02:45:07.638181Z","iopub.execute_input":"2023-11-11T02:45:07.638673Z","iopub.status.idle":"2023-11-11T02:45:07.661486Z","shell.execute_reply.started":"2023-11-11T02:45:07.638621Z","shell.execute_reply":"2023-11-11T02:45:07.660460Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Observation result:\n- There are no duplicates in data","metadata":{"id":"MHmuACVUN96i"}},{"cell_type":"markdown","source":"Now let's go inspect the summary statistics for numerical data first.","metadata":{}},{"cell_type":"code","source":"# Numerical summary statistics\ndf.describe()","metadata":{"id":"ZjO2-j-6131n","outputId":"7b0a245e-434a-488e-8666-8c043010bc2a","execution":{"iopub.status.busy":"2023-11-11T02:45:07.663891Z","iopub.execute_input":"2023-11-11T02:45:07.664730Z","iopub.status.idle":"2023-11-11T02:45:07.704824Z","shell.execute_reply.started":"2023-11-11T02:45:07.664673Z","shell.execute_reply":"2023-11-11T02:45:07.703646Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Observation results:\n- The `Discount_offered` variable indicate a right-skewed distribution (Need to confirm later on with histograms), high variance, and potential outlier based on the max and 75% percentile distance\n- `Prior_purchases` variable has potential outliers\n- `Weight_in_gms` variable indicate a left-skewed distribution\n- Other columns looks 'normal' enough based on their numbers\n\nThen, let's move on to the categorical data summary statistics.","metadata":{"id":"QQd2SDR6zaBm"}},{"cell_type":"code","source":"# Categorical summary statistics\ndf.describe(include = 'object').drop(columns = 'ID')","metadata":{"id":"n4X_mGLjBZFP","outputId":"47785f0e-370e-4e1a-c9d9-1adae146db69","execution":{"iopub.status.busy":"2023-11-11T02:45:07.706293Z","iopub.execute_input":"2023-11-11T02:45:07.706658Z","iopub.status.idle":"2023-11-11T02:45:07.764787Z","shell.execute_reply.started":"2023-11-11T02:45:07.706624Z","shell.execute_reply":"2023-11-11T02:45:07.763335Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Observation results:\n- Most products (about 68%) are delivered using the ship method.\n- Most deliveries are late (about 60%).\n- All columns seem fine based on their numbers (no strange or catchy values).","metadata":{"id":"szQTrzpbBkV5"}},{"cell_type":"markdown","source":"## Univariate Analysis\nFirst thing first, we're going to explore the features individually, inspecting their distributions. Before diving further, let's create two variables that will separate categorical and numerical variables","metadata":{"id":"NFvMUhFsc-BD"}},{"cell_type":"code","source":"# Separating between categorical and numerical data\ncats = df.select_dtypes(include = 'object').columns.tolist()\nnums = df.select_dtypes(exclude = 'object').columns.tolist()","metadata":{"id":"500yRoFkN39t","execution":{"iopub.status.busy":"2023-11-11T02:45:07.771016Z","iopub.execute_input":"2023-11-11T02:45:07.771412Z","iopub.status.idle":"2023-11-11T02:45:07.782741Z","shell.execute_reply.started":"2023-11-11T02:45:07.771380Z","shell.execute_reply":"2023-11-11T02:45:07.781372Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Because we will not use the `ID` variable, we'll remove it from the list of categoricals","metadata":{"id":"iHFqmpNLN73C"}},{"cell_type":"code","source":"# Removing the ID variable from categoricals\ncats = cats[1:]","metadata":{"id":"xQhpumueN-eo","execution":{"iopub.status.busy":"2023-11-11T02:45:07.784671Z","iopub.execute_input":"2023-11-11T02:45:07.785496Z","iopub.status.idle":"2023-11-11T02:45:07.792623Z","shell.execute_reply.started":"2023-11-11T02:45:07.785450Z","shell.execute_reply":"2023-11-11T02:45:07.791061Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, let's check the distribution of numerical features using a box plot.","metadata":{}},{"cell_type":"code","source":"# Checking the distribution of each numerical column with a boxplot\nplt.figure(figsize = (5, 7))\n\nfor i in range (0, len(nums)):\n  plt.subplot(2,3, i+1)\n  sns.boxplot(y=df[nums[i]], orient = 'v')\n\nplt.tight_layout()","metadata":{"id":"KbVp3STFdCyo","outputId":"c8c3b024-845e-465f-91e3-743b3a7e830c","execution":{"iopub.status.busy":"2023-11-11T02:45:07.794685Z","iopub.execute_input":"2023-11-11T02:45:07.795705Z","iopub.status.idle":"2023-11-11T02:45:08.768966Z","shell.execute_reply.started":"2023-11-11T02:45:07.795657Z","shell.execute_reply":"2023-11-11T02:45:08.768094Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Observation results:\n\n- The numerical features that has outliers are `Prior_Purchases` and `Dicount_offered`.\n- The `Discount_offered` feature has the most outliers.\n- No outliers found in other features.\n\nWe can further check the distribution using a histogram.","metadata":{"id":"P17Y9BDpEHLa"}},{"cell_type":"code","source":"# Checking the distribution of each numerical column with histogram\nfor i in range (0, len(nums)):\n  plt.subplot(3,2, i+1)\n  sns.kdeplot(x=df[nums[i]])\n  sns.histplot(kde = True, x = df[nums[i]])\n  plt.tight_layout()","metadata":{"id":"R-AWqaAoXf0s","outputId":"c0f6b96a-ce24-4850-e821-a90b06c12e96","execution":{"iopub.status.busy":"2023-11-11T02:45:08.770434Z","iopub.execute_input":"2023-11-11T02:45:08.770993Z","iopub.status.idle":"2023-11-11T02:45:12.152167Z","shell.execute_reply.started":"2023-11-11T02:45:08.770959Z","shell.execute_reply":"2023-11-11T02:45:12.150435Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Observation results:\n- The `Cost_of_the_product` distribution is approximately normal\n- `Weight_in_gms` feature have a bimodal distribution.\n- The `Discount_offered` feature has a positively skewed distribution.\n\nThe following variables contains discrete values and needed their own bins to be shown appropriately:\n- `Customer_care_calls`\n- `Prior_purchases`","metadata":{"id":"o_oOlTSIECoU"}},{"cell_type":"code","source":"custom_bins = ['Customer_care_calls', 'Prior_purchases']\nplt.figure(figsize = (8, 4))\n\nfor i in range(0, len(custom_bins)):\n    bins = np.arange(0, df[custom_bins[i]].max() + 1.5) - 0.5\n    plt.subplot(1, 2, i + 1)\n    ax = sns.histplot(x = df[custom_bins[i]], bins = bins, color = 'darkblue')\n    plt.xticks(bins + 0.5)\n    sns.despine(right = True)\n    plt.bar_label(ax.containers[0], label_type = 'edge', fontweight = 'normal', size = 7)\n    plt.title(f'Distribution of {custom_bins[i]}')\n\nplt.tight_layout()\nplt.show()","metadata":{"id":"SHs1upV7D0_O","outputId":"9d505c75-af08-4503-e1d4-d255df98436a","execution":{"iopub.status.busy":"2023-11-11T02:45:12.154209Z","iopub.execute_input":"2023-11-11T02:45:12.154729Z","iopub.status.idle":"2023-11-11T02:45:13.009684Z","shell.execute_reply.started":"2023-11-11T02:45:12.154668Z","shell.execute_reply":"2023-11-11T02:45:13.008511Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Observation results:\n\n- The `Customer_care_calls` feature has a normal distribution, The highest customer care calls is at 4 calls with 3557 customers.\n- The `Prior_purchases` feature has a positively skewed distribution, the highest `Prior_purchases` is at 3 prior purchases with 3955 customers.\n\nWe can additionally check the skewness in numbers here.","metadata":{"id":"QU4B4btvEOJY"}},{"cell_type":"code","source":"# Skewness in numbers\ndf.skew(numeric_only = True).sort_values(ascending = False)","metadata":{"id":"8jQ73RX8Kd91","outputId":"05a8ecca-e34b-4264-e708-e2c58a28bb58","execution":{"iopub.status.busy":"2023-11-11T02:45:13.011392Z","iopub.execute_input":"2023-11-11T02:45:13.012529Z","iopub.status.idle":"2023-11-11T02:45:13.025728Z","shell.execute_reply.started":"2023-11-11T02:45:13.012484Z","shell.execute_reply":"2023-11-11T02:45:13.024448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can further confirm that the following features have skewed distribution:\n1. `Discount_offered`\n2. `Prior_purchases`\n\nThen, let's check the distribution of values across categorical features.","metadata":{"id":"BCo-xVUjKhCC"}},{"cell_type":"code","source":"plt.figure(figsize = (9, 14))\nplotnumber = 1\n\nfor i in range(len(cats)):\n    if plotnumber <= 8:\n        if cats[i] == 'Warehouse_block':\n            ax = plt.subplot(4, 2, plotnumber)\n            sns.countplot(x = cats[i], data = df , ax = ax, order = sorted(df['Warehouse_block'].unique().tolist()))\n            plt.bar_label(ax.containers[0], label_type = 'edge', fontweight = 'normal')\n            plt.title(f\"\\n{cats[i]} Value Counts\\n\", fontsize = 12)\n            plt.margins(x = 0.05, y = 0.4)\n        elif cats[i] == 'Customer_rating':\n            ax = plt.subplot(4, 2, plotnumber)\n            sns.countplot(x = cats[i], data = df , ax = ax, order = sorted(df['Customer_rating'].unique().tolist()))\n            plt.bar_label(ax.containers[0], label_type = 'edge', fontweight = 'normal')\n            plt.title(f\"\\n{cats[i]} Value Counts\\n\", fontsize = 12)\n            plt.margins(x = 0.05, y = 0.4)\n        else:\n            ax = plt.subplot(4, 2, plotnumber)\n            sns.countplot(x = cats[i], data = df , ax = ax)\n            plt.bar_label(ax.containers[0], label_type = 'edge', fontweight = 'normal')\n            plt.title(f\"\\n{cats[i]} Value Counts\\n\", fontsize = 12)\n            plt.margins(x = 0.05, y = 0.4)\n    plotnumber += 1\n\nplt.tight_layout()\nplt.show()","metadata":{"id":"e2pFT85eYGIy","outputId":"c91ef02d-0a3a-4aa5-bd6a-97142596a82c","execution":{"iopub.status.busy":"2023-11-11T02:45:13.027410Z","iopub.execute_input":"2023-11-11T02:45:13.028044Z","iopub.status.idle":"2023-11-11T02:45:14.645675Z","shell.execute_reply.started":"2023-11-11T02:45:13.027990Z","shell.execute_reply":"2023-11-11T02:45:14.644468Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Observation results:\n- Most shipments comes from `Warehouse_block` F.\n  - Strangely, warehouse block E is missing from the data\n- Based on the `Mode_of_Shipment`, the majority of order is delivered using the ship method.\n- `Product_importance` shows that the most shipments are \"Low\", followed by \"Medium\" and the least are \"High\".\n- There are fewer on time deliveries (4436) than late deliveries (6563) based on the `Delivery_status` feature.\n- The distribution of values inside `Customer_rating` and `Gender` is similar (no dominant values).","metadata":{"id":"GH_Gke48Eha7"}},{"cell_type":"markdown","source":"## Multivariate Analysis\nAfter getting a sense of each features' distributions, we can now inspect the relationship between each features, especially to find a meaningful relationship between featues and our target variable (`Delivery_status`).\n\n### Numerical Features vs. Target\n#### Correlation Heatmap","metadata":{"id":"h56Vt6qMdFVv"}},{"cell_type":"code","source":"# Excluding the ID from correlation\ndf_corr = df.drop(columns = 'ID')\n\n# Changing the `Delivery_status` temporary for showing correlation\ndf_corr['Delivery_status'] = df_corr['Delivery_status'].astype('int')\n\n# Correlation heatmap\nsns.heatmap(df_corr.corr(numeric_only = True), cmap = 'Blues', annot = True, fmt = '.3f')\nplt.show()","metadata":{"id":"FV-X0QI9SwQC","outputId":"63c5c587-f665-4a7d-d64a-47e522aebbd3","execution":{"iopub.status.busy":"2023-11-11T02:45:14.647755Z","iopub.execute_input":"2023-11-11T02:45:14.648678Z","iopub.status.idle":"2023-11-11T02:45:15.136067Z","shell.execute_reply.started":"2023-11-11T02:45:14.648631Z","shell.execute_reply":"2023-11-11T02:45:15.134595Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Observation results:\n- `Discount_offered` has moderate positive relationship with our target (r = 0.4), meaning that higher discount lead to higher probability of late deliveries. This feature may need to be retained for modeling.\n- `Weight_in_gms` has weak negative relationship with our target (r = -0.27), meaning that the heavier a product is, the higher the probability of late deliveries. This feature may need to be retained for modeling.\n\nAs for features vs. features, the correlation doesn't show any strong relationships (above 0.8). Let's investigate further using VIF\n\n#### Multicollinearity Check with VIF","metadata":{"id":"H7Rxh-3xTNcn"}},{"cell_type":"code","source":"from statsmodels.stats.outliers_influence import variance_inflation_factor\n\n# Independent variables set\nX = df[nums]\n\n# VIF dataframe\nvif_data = pd.DataFrame()\nvif_data[\"feature\"] = X.columns\n\n# calculating VIF for each feature\nvif_data[\"VIF\"] = [variance_inflation_factor(X.values, i)\n                          for i in range(len(X.columns))]\n\nvif_data.sort_values('VIF', ascending = False)","metadata":{"id":"lOhAaB1CNcPw","outputId":"d03593f7-e2c3-46ad-9da5-57f020408115","execution":{"iopub.status.busy":"2023-11-11T02:45:15.137897Z","iopub.execute_input":"2023-11-11T02:45:15.138455Z","iopub.status.idle":"2023-11-11T02:45:15.207927Z","shell.execute_reply.started":"2023-11-11T02:45:15.138408Z","shell.execute_reply":"2023-11-11T02:45:15.206077Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Generally, VIF above 10 means that particular variable can't be used for modelling and the multicollinearity is severe. As we can see from the above results, we have several features that will cause multicollinearity problem, namely:\n1. `Cost_of_the_Product`\n2. `Customer_care_calls`\n\nTo determine which one to remove, we'll compare their strength to the target variable, `Delivery_status`:\n1. `Cost_of_the_Product` = -0.074\n2. `Customer_care_calls` = -0.067\n\nBecause `Customer_care_calls` have multicollinearity problem and weaker correlation with the target, we'll drop `Customer_care_calls` later in the preprocessing stage\n\nLet's continue the multivariate exploration with a pair plot","metadata":{"id":"pgRPqKyBR8IR"}},{"cell_type":"markdown","source":"#### Pair Plot","metadata":{}},{"cell_type":"code","source":"# Pair plot\nsns.pairplot(data = df, diag_kind = 'kde', hue = 'Delivery_status', hue_order = ['0', '1'])","metadata":{"id":"fbtbbhvxdLnP","outputId":"10252979-e1bb-4359-f173-e1f99418dd7b","execution":{"iopub.status.busy":"2023-11-11T02:45:15.211078Z","iopub.execute_input":"2023-11-11T02:45:15.211613Z","iopub.status.idle":"2023-11-11T02:45:47.900411Z","shell.execute_reply.started":"2023-11-11T02:45:15.211565Z","shell.execute_reply":"2023-11-11T02:45:47.899105Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The late product shipment tend to cluster at:\n- Higher amount of discount offered\n- Several medium weight range and lower weight range\n\nWe can further explore to the boxplot of numerical features vs. target","metadata":{"id":"AEUQbFv4NlWX"}},{"cell_type":"markdown","source":"#### Box Plot","metadata":{}},{"cell_type":"code","source":"# Box plot\nplt.figure(figsize = (7, 7))\n\nfor i in range(0, len(nums)):\n    plt.subplot(2, 3, i + 1)\n    ax = sns.boxplot(x = df['Delivery_status'].sort_values(), y = df[nums[i]])\n    sns.despine(right = True)\n\nplt.tight_layout()\nplt.show()","metadata":{"id":"4J5bsIYjNsXo","outputId":"21d6c09a-4202-4b4c-a65d-7af88bdf311e","execution":{"iopub.status.busy":"2023-11-11T02:45:47.902490Z","iopub.execute_input":"2023-11-11T02:45:47.903751Z","iopub.status.idle":"2023-11-11T02:45:49.014720Z","shell.execute_reply.started":"2023-11-11T02:45:47.903708Z","shell.execute_reply":"2023-11-11T02:45:49.013426Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, we can further confirm that:\n- Late deliveries always happened at a higher amount of discount offered (> 10% of discount)\n- Late deliveries always occured at the weight range of 1000 - 4000 gr and more than 6000 gr\n- Other numerical variables doesn't show much difference between on time and late deliveries\n\n**Indication of good features list**\n1. `Discount_offered`\n2. `Weight_in_gms`\n\nNow, let's move on to find the relationships between categorical features and our target (`Delivery_status`).","metadata":{"id":"21Vgt50IOxz5"}},{"cell_type":"markdown","source":"### Categorical (Warehouse Block) vs. Target\nBecause we're comparing different attributes of on-time vs. late deliveries. We will base our comparison with frequency and proportion.","metadata":{"id":"-_Zc-HAPPIQ2"}},{"cell_type":"code","source":"plt.figure(figsize = (10, 5))\n\n# Frequency base\nfig, axes = plt.subplots(1, 2, figsize=(10, 5))\nplt.subplots_adjust(right = 1.2)\nsns.despine(right = True)\n\nsns.countplot(ax = axes[0], x = df['Warehouse_block'], hue = df['Delivery_status'].sort_values(), order = sorted(df['Warehouse_block'].unique().tolist()))\n\naxes[0].bar_label(axes[0].containers[0], label_type = 'edge', fontweight = 'normal', size = 8)\naxes[0].bar_label(axes[0].containers[1], label_type = 'edge', fontweight = 'normal', size = 8)\n\naxes[0].set(ylabel = 'Frequency', xlabel = 'Warehouse Block')\naxes[0].set_title('Distribution of Delivery Status by Warehouse Block', y = 1.05)\naxes[0].legend(['On Time', 'Late'], loc = 0, ncol = 1, title = 'Delivery Status', borderaxespad = 0.8)\n\n# Proportion base\nwb_p = df.groupby(['Warehouse_block', 'Delivery_status'])['Delivery_status'].count().reset_index(name = 'Freq')\nwb_p['Prop'] = round(wb_p['Freq'] / wb_p.groupby('Warehouse_block')['Freq'].transform(sum) * 100, 2)\nwb_p.pivot_table(index='Warehouse_block', columns='Delivery_status', values='Prop').plot(kind='bar', stacked=True, ax=axes[1])\n\naxes[1].bar_label(axes[1].containers[0], label_type='center', fontweight='light', fmt='%.0f%%')\naxes[1].bar_label(axes[1].containers[1], label_type='center', fontweight='light', fmt='%.0f%%')\n\naxes[1].set_title('Proportion of Delivery Status by Warehouse Block', y = 1.05)\naxes[1].set(ylabel = 'Proportion (%)', xlabel = 'Warehouse Block')\naxes[1].set_xticks(ticks = axes[1].get_xticks(), labels = axes[1].get_xticklabels(), rotation = 0)\n\naxes[1].get_legend().set_visible(False)\n\nplt.show()","metadata":{"id":"s9gGxLGOPNGW","outputId":"d9502af7-4637-474e-efe9-20cb6eaaeaf6","execution":{"iopub.status.busy":"2023-11-11T02:45:49.016863Z","iopub.execute_input":"2023-11-11T02:45:49.017705Z","iopub.status.idle":"2023-11-11T02:45:49.726784Z","shell.execute_reply.started":"2023-11-11T02:45:49.017655Z","shell.execute_reply":"2023-11-11T02:45:49.722189Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Observation results:\n\n- All deliveries mostly are late across all warehouse blocks\n- When we compare the similar late and on-time delivery proportions for each warehouse block, we can see that the warehouse block itself does not significantly influence whether a shipment will be late or on time.","metadata":{"id":"6Y1dqYmvPPKU"}},{"cell_type":"markdown","source":"### Categorical (Mode of Shipment) vs. Target","metadata":{"id":"MWb8lH_jPUVI"}},{"cell_type":"code","source":"plt.figure(figsize = (10, 5))\n\n# Frequency base\nfig, axes = plt.subplots(1, 2, figsize=(10, 5))\nplt.subplots_adjust(right = 1.2)\nsns.despine(right = True)\n\nsns.countplot(ax = axes[0], x = df['Mode_of_Shipment'], hue = df['Delivery_status'].sort_values(), order = ['Flight', 'Road', 'Ship'])\n\naxes[0].bar_label(axes[0].containers[0], label_type = 'edge', fontweight = 'normal')\naxes[0].bar_label(axes[0].containers[1], label_type = 'edge', fontweight = 'normal')\n\naxes[0].set(ylabel = 'Frequency', xlabel = 'Mode of Shipment')\naxes[0].set_title('Distribution of Delivery Status by Mode of Shipment', y = 1.05)\naxes[0].legend(['On Time', 'Late'], loc = 0, ncol = 1, title = 'Delivery Status', borderaxespad = 0.8)\n\n# Proportion base\nmos_p = df.groupby(['Mode_of_Shipment', 'Delivery_status'])['Delivery_status'].count().reset_index(name = 'Freq')\nmos_p['Prop'] = round(mos_p['Freq'] / mos_p.groupby('Mode_of_Shipment')['Freq'].transform(sum) * 100, 2)\nmos_p.pivot_table(index='Mode_of_Shipment', columns='Delivery_status', values='Prop').plot(kind='bar', stacked=True, ax=axes[1])\n\naxes[1].bar_label(axes[1].containers[0], label_type='center', fontweight='light', fmt='%.0f%%')\naxes[1].bar_label(axes[1].containers[1], label_type='center', fontweight='light', fmt='%.0f%%')\n\naxes[1].set_title('Proportion of Delivery Status by Mode of Shipment', y = 1.05)\naxes[1].set(ylabel = 'Proportion (%)', xlabel = 'Mode of Shipment')\naxes[1].set_xticks(ticks = axes[1].get_xticks(), labels = axes[1].get_xticklabels(), rotation = 0)\n\naxes[1].get_legend().set_visible(False)\n\nplt.show()","metadata":{"id":"CAmZtooFPYn6","outputId":"8573457b-69f5-45ac-f181-d504e9cccf77","execution":{"iopub.status.busy":"2023-11-11T02:45:49.728419Z","iopub.execute_input":"2023-11-11T02:45:49.729402Z","iopub.status.idle":"2023-11-11T02:45:50.300175Z","shell.execute_reply.started":"2023-11-11T02:45:49.729354Z","shell.execute_reply":"2023-11-11T02:45:50.298989Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Observation results:\n- All deliveries mostly are late across all mode of shipment.\n- We can see that mode of shipment does not significantly impact delivery lateness, as each mode of shipment shows a similar rate of on time and late deliveries.","metadata":{"id":"NIupn8pEPbKr"}},{"cell_type":"markdown","source":"### Categorical (Customer Rating) vs. Target","metadata":{"id":"6570s2TsSONI"}},{"cell_type":"code","source":"plt.figure(figsize = (10, 5))\n\n# Frequency base\nfig, axes = plt.subplots(1, 2, figsize=(10, 5))\nplt.subplots_adjust(right = 1.2)\nsns.despine(right = True)\n\nsns.countplot(ax = axes[0], x = df['Customer_rating'], hue = df['Delivery_status'].sort_values(),\n              order = sorted(df['Customer_rating'].unique().tolist()))\n\naxes[0].bar_label(axes[0].containers[0], label_type = 'edge', fontweight = 'normal')\naxes[0].bar_label(axes[0].containers[1], label_type = 'edge', fontweight = 'normal')\n\naxes[0].set(ylabel = 'Frequency', xlabel = 'Customer Rating')\naxes[0].set_title('Distribution of Delivery Status by Customer Rating', y = 1.05)\naxes[0].legend(['On Time', 'Late'], loc = 0, ncol = 1, title = 'Delivery Status', borderaxespad = 0.8)\naxes[0].margins(0.1, 0.5)\n\n# Proportion base\ncr_p = df.groupby(['Customer_rating', 'Delivery_status'])['Delivery_status'].count().reset_index(name = 'Freq')\ncr_p['Prop'] = round(cr_p['Freq'] / cr_p.groupby('Customer_rating')['Freq'].transform(sum) * 100, 2)\ncr_p.pivot_table(index='Customer_rating', columns='Delivery_status', values='Prop').plot(kind='bar', stacked=True, ax=axes[1])\n\naxes[1].bar_label(axes[1].containers[0], label_type='center', fontweight='light', fmt='%.0f%%')\naxes[1].bar_label(axes[1].containers[1], label_type='center', fontweight='light', fmt='%.0f%%')\n\naxes[1].set_title('Proportion of Delivery Status by Customer Rating', y = 1.05)\naxes[1].set(ylabel = 'Proportion (%)', xlabel = 'Customer Rating')\naxes[1].set_xticks(ticks = axes[1].get_xticks(), labels = axes[1].get_xticklabels(), rotation = 0)\n\naxes[1].get_legend().set_visible(False)\n\nplt.show()","metadata":{"id":"WZvRzVjWSR-P","outputId":"06708414-31be-425a-d583-251447b1a14c","execution":{"iopub.status.busy":"2023-11-11T02:45:50.301951Z","iopub.execute_input":"2023-11-11T02:45:50.302711Z","iopub.status.idle":"2023-11-11T02:45:50.973576Z","shell.execute_reply.started":"2023-11-11T02:45:50.302668Z","shell.execute_reply":"2023-11-11T02:45:50.972263Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Observation results:\n- All deliveries mostly are late across all customer ratings.\n- Based on the proportions, we can see that there is not much differences between customer rating of 1 to 5. This means that customer rating doesn't influence the status of a delivery (whether it will late or not).","metadata":{"id":"L3b-XdlASTt6"}},{"cell_type":"markdown","source":"### Categorical (Product Importance) vs. Target","metadata":{"id":"bivapF2-PcMq"}},{"cell_type":"code","source":"plt.figure(figsize = (10, 5))\n\n# Frequency base\nfig, axes = plt.subplots(1, 2, figsize=(10, 5))\nplt.subplots_adjust(right = 1.2)\nsns.despine(right = True)\n\nsns.countplot(ax = axes[0], x = df['Product_importance'], hue = df['Delivery_status'].sort_values(), order = ['low', 'medium', 'high'])\n\naxes[0].bar_label(axes[0].containers[0], label_type = 'edge', fontweight = 'normal')\naxes[0].bar_label(axes[0].containers[1], label_type = 'edge', fontweight = 'normal')\n\naxes[0].set(ylabel = 'Frequency', xlabel = 'Product Importance')\naxes[0].set_title('Distribution of Delivery Status by Product Importance', y = 1.05)\naxes[0].legend(['On Time', 'Late'], loc = 0, ncol = 1, title = 'Delivery Status', borderaxespad = 0.8)\n\n# Proportion base\npi_p = df.groupby(['Product_importance', 'Delivery_status'])['Delivery_status'].count().reset_index(name = 'Freq')\npi_p['Prop'] = round(pi_p['Freq'] / pi_p.groupby('Product_importance')['Freq'].transform(sum) * 100, 2)\npi_p['Product_importance'] = pd.Categorical(pi_p['Product_importance'], ['low', 'medium', 'high'])\npi_p.sort_values('Product_importance', inplace = True)\npi_p.pivot_table(index='Product_importance', columns='Delivery_status', values='Prop').plot(kind='bar', stacked=True, ax=axes[1])\n\naxes[1].bar_label(axes[1].containers[0], label_type='center', fontweight='light', fmt='%.0f%%')\naxes[1].bar_label(axes[1].containers[1], label_type='center', fontweight='light', fmt='%.0f%%')\n\naxes[1].set_title('Proportion of Delivery Status by Product Importance', y = 1.05)\naxes[1].set(ylabel = 'Proportion (%)', xlabel = 'Product Importance')\naxes[1].set_xticks(ticks = axes[1].get_xticks(), labels = axes[1].get_xticklabels(), rotation = 0)\n\naxes[1].get_legend().set_visible(False)\n\nplt.show()","metadata":{"id":"cHhJGNVDPfWb","outputId":"3b1afaf4-681e-4be9-d8b9-96b985d4b035","execution":{"iopub.status.busy":"2023-11-11T02:45:50.975360Z","iopub.execute_input":"2023-11-11T02:45:50.975782Z","iopub.status.idle":"2023-11-11T02:45:51.909481Z","shell.execute_reply.started":"2023-11-11T02:45:50.975748Z","shell.execute_reply":"2023-11-11T02:45:51.908036Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Observation results:\n- All deliveries mostly are late across all product importance.\n- Looking at the proportions, it's visible that products categorized as high importance are more likely to experience late deliveries compared to those categorized as low and medium importance. This indicates that product importance is one of the factors influencing whether a delivery will be late or not.","metadata":{"id":"zUp1ngS5PhWb"}},{"cell_type":"markdown","source":"### Categorical (Gender) vs. Target","metadata":{"id":"Pm1ieLlePkWx"}},{"cell_type":"code","source":"plt.figure(figsize = (10, 5))\n\n# Frequency base\nfig, axes = plt.subplots(1, 2, figsize=(10, 5))\nplt.subplots_adjust(right = 1.2)\nsns.despine(right = True)\n\nsns.countplot(ax = axes[0], x = df['Gender'], hue = df['Delivery_status'].sort_values())\n\naxes[0].bar_label(axes[0].containers[0], label_type = 'edge', fontweight = 'normal')\naxes[0].bar_label(axes[0].containers[1], label_type = 'edge', fontweight = 'normal')\n\naxes[0].set(ylabel = 'Frequency', xlabel = 'Gender')\naxes[0].set_title('Distribution of Delivery Status by Gender', y = 1.05)\naxes[0].legend(['On Time', 'Late'], loc = 0, ncol = 1, title = 'Delivery Status', borderaxespad = 0.8)\naxes[0].margins(0, 0.5)\n\n# Proportion base\ng_p = df.groupby(['Gender', 'Delivery_status'])['Delivery_status'].count().reset_index(name = 'Freq')\ng_p['Prop'] = round(g_p['Freq'] / g_p.groupby('Gender')['Freq'].transform(sum) * 100, 2)\ng_p.pivot_table(index='Gender', columns='Delivery_status', values='Prop').plot(kind='bar', stacked=True, ax=axes[1])\n\naxes[1].bar_label(axes[1].containers[0], label_type='center', fontweight='light', fmt='%.0f%%')\naxes[1].bar_label(axes[1].containers[1], label_type='center', fontweight='light', fmt='%.0f%%')\n\naxes[1].set_title('Proportion of Delivery Status by Gender', y = 1.05)\naxes[1].set(ylabel = 'Proportion (%)', xlabel = 'Gender')\naxes[1].set_xticks(ticks = axes[1].get_xticks(), labels = axes[1].get_xticklabels(), rotation = 0)\n\naxes[1].get_legend().set_visible(False)\n\nplt.show()","metadata":{"id":"O6SsRcR5PmeJ","outputId":"7dcd6bca-9ec6-4df7-e310-0f486072d68d","execution":{"iopub.status.busy":"2023-11-11T02:45:51.911181Z","iopub.execute_input":"2023-11-11T02:45:51.911547Z","iopub.status.idle":"2023-11-11T02:45:52.448021Z","shell.execute_reply.started":"2023-11-11T02:45:51.911518Z","shell.execute_reply":"2023-11-11T02:45:52.446853Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Observation results:\n- All deliveries mostly are late across all genders\n- We can see that there's no significant effect of gender to late deliveries because the proportion of late and on time deliveries across all gender is highly similar\n\n**Indication of good features list**\n\n1. `Discount_offered`\n2. `Weight_in_gms`\n3. `Product_importance`\n\n**What is the correlation between features, are there any interesting patterns? What needs to be done with that feature?**\n\n- Based on the correlation heatmap, we could see that several features have weak to moderate relationship with other features. Further checked with VIF, we'll drop some features later on to eliminate the multicollinearity problem.","metadata":{"id":"_rW__no9Po_d"}},{"cell_type":"markdown","source":"## Business Insights & Recommendations (EDA Based)\nAfter the exploration phase above, we find several factors that are associated with late deliveries in this dataset.\n\n### Business Insight","metadata":{"id":"AmdOCDTvdNPt"}},{"cell_type":"code","source":"ax = sns.boxplot(x = df['Delivery_status'].sort_values(), y = df['Discount_offered'], palette = ['gray', 'darkblue'])\nsns.despine(right = True)\n\nax.axhline(df[df['Delivery_status'] == '0']['Discount_offered'].max(), color = '#C7A300', ls = '--', lw = 2)\n\nplt.title('Late deliveries are happening at a higher discount rate', weight = 'bold', c = 'darkblue', x = 0.518, y = 1.14, size = 14)\nax.annotate(xytext = (-0.4, 35), xy = (0.6, 20), text = 'Tend to late above \\n10% discount', color = 'darkblue', arrowprops = dict(arrowstyle = '->', color = '#404040'))\nplt.text(s = 'Customers who applied high rate of discounts on their orders are \\nexperiencing late deliveries', c = '#404040', x = -0.68, y = 72)\n\nax.set_xticklabels(['On Time', 'Late'], color = '#404040')\nplt.setp(ax.get_yticklabels(), color = \"gray\", size = 9)\nax.tick_params(color = \"gray\", bottom = False)\n\nax.set_xlabel('')\nax.set_ylabel('Discount Offered (%)', color = \"gray\", y = 0.8)\n\nfor spine in ax.spines.values():\n    spine.set_edgecolor(\"gray\")\n\nplt.show()","metadata":{"id":"ajcURDJNZeYl","outputId":"e39f99ca-84a1-4cba-ed59-5abff41a36d0","execution":{"iopub.status.busy":"2023-11-11T02:45:52.449604Z","iopub.execute_input":"2023-11-11T02:45:52.450250Z","iopub.status.idle":"2023-11-11T02:45:52.810769Z","shell.execute_reply.started":"2023-11-11T02:45:52.450218Z","shell.execute_reply":"2023-11-11T02:45:52.809297Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Customers who ordered and applied a discount of more than 10% are experiencing late deliveries. This means that a higher amount of discount is associated with late deliveries. The higher the discount offered, the higher the probability of the delivery status will be late. Thus, recommendations regarding the amount of discount offered need to be given.\n\nHere are several assumptions regarding the lateness in higher discounted shipments:\n1. **Resource Allocation**: The company may prioritize orders with lower discounts, as they might generate higher profit margins. This could result in fewer resources and attention given to orders with higher discounts, leading to delays in processing and shipment.\n2. **Limited Supply**: Orders with higher discounts may be for products that are limited in supply. This could lead to possible inventory shortages and cause delays.","metadata":{"id":"pk6ioOU65R8k"}},{"cell_type":"code","source":"# Proportion base\n\nlate_pi_p = pi_p[pi_p['Delivery_status'] == '1'][['Product_importance', 'Prop']].rename(columns = {'Prop' : 'Late_prop'})\n\nax = sns.barplot(x = late_pi_p['Product_importance'], y = late_pi_p['Late_prop'],\n                 palette = ['darkblue', 'gray', 'gray'], order = ['high', 'medium', 'low'])\nsns.despine(right = True)\n\nbarlab = plt.bar_label(ax.containers[0], label_type = 'center', fontweight = 'normal', fmt = '%.0f%%', color = 'darkgray')\nbarlab[0].set(color = '#C7A300', weight = 'bold')\n\nplt.setp(ax.get_yticklabels(), color = \"gray\", size = 9)\nax.tick_params(color = \"gray\", bottom = False)\n\nax.set_xticklabels(['High', 'Medium', 'Low'], color = '#404040')\nax.set_xlabel('Product Importance', color = \"gray\")\nax.set_ylabel('Proportion (%)', color = \"gray\", y = 0.87)\n\nplt.title('High-importance products face delivery delays more', weight = 'bold', c = 'darkblue', x = 0.42, y = 1.16, size = 14)\nplt.text(s = 'than other product importances. This seems counterintuitive since items \\nmarked as high importance should typically be prioritized for on-time delivery',\n         c = '#404040', x = -0.97, y = 73)\n\nfor spine in ax.spines.values():\n    spine.set_edgecolor(\"gray\")\n\nplt.show()","metadata":{"id":"5K6Z9xswZiuo","outputId":"1b0a06ac-a95b-43ed-ad2d-8e3a42c6d1f9","execution":{"iopub.status.busy":"2023-11-11T02:45:52.812405Z","iopub.execute_input":"2023-11-11T02:45:52.812834Z","iopub.status.idle":"2023-11-11T02:45:53.113273Z","shell.execute_reply.started":"2023-11-11T02:45:52.812799Z","shell.execute_reply":"2023-11-11T02:45:53.111709Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"High importance products category tend to experience more late deliveries than other product importance levels. It is important to identify and overcome the causes of this lateness in detail (need more data about the detail about the characteristics of \"High\" importance goods) to improve the delivery performance of high importance category products\n\nHere are several assumptions regarding the lateness in high product importance:\n1. **Complex Handling**: High importance orders may require more complex handling, like specialized packaging or stricter quality checks, which can lead to delays in the shipment process.","metadata":{"id":"YyJD8k87hf9m"}},{"cell_type":"code","source":"ax = sns.boxplot(x = df['Delivery_status'].sort_values(), y = df['Weight_in_gms'], palette = ['gray', 'darkblue'])\nsns.despine(right = True)\n\nplt.title('Several weight ranges are experiencing late deliveries', weight = 'bold', c = 'darkblue', x = 0.478, y = 1.1, size = 14)\nplt.text(s = 'Products around the highlighted ranges always running late', c = '#404040', x = -0.75, y = 8700)\n\nax.annotate(xytext = (1.43, 4000), xy = (1.43, 900), text = '', arrowprops = dict(arrowstyle = '-', color = '#C7A300'))\nplt.text(s = 'Late deliveries \\naround 1000-4000 gms', x = 1.46, y = 2300, color = 'darkblue')\nax.annotate(xytext = (1.43, 8000), xy = (1.43, 6000), text = '', arrowprops = dict(arrowstyle = '-', color = '#C7A300'))\nplt.text(s = 'Late deliveries \\naround > 6000 gms', x = 1.46, y = 6900, color = 'darkblue')\n\nax.set_xticklabels(['On Time', 'Late'], color = '#404040')\nplt.setp(ax.get_yticklabels(), color = \"gray\", size = 9)\nax.tick_params(color = \"gray\", bottom = False)\n\nax.set_xlabel('')\nax.set_ylabel('Weight (gr)', color = \"gray\", y = 0.89)\n\nfor spine in ax.spines.values():\n    spine.set_edgecolor(\"gray\")\n\nplt.show()","metadata":{"id":"3RmN7NUAZ6Wj","outputId":"4c961921-a986-4383-bddd-073862a483bb","execution":{"iopub.status.busy":"2023-11-11T02:45:53.123315Z","iopub.execute_input":"2023-11-11T02:45:53.123762Z","iopub.status.idle":"2023-11-11T02:45:53.466258Z","shell.execute_reply.started":"2023-11-11T02:45:53.123727Z","shell.execute_reply":"2023-11-11T02:45:53.465367Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Products/goods weighing around 1000 - 4000 gr and more than 6000 gr experience more late deliveries than other weight ranges. Further analysis needs to be carried out to identify specific influencing factors to improve heavy product delivery performance.\n\nNevertheless, the lateness seems to be more concentrated around lighter products' weight that we can see from the center of the distribution.\n\n*Side note: The late weight range starts from 1000 gr because on-time deliveries at around 1000-2000gr is considered an outlier, meaning that there's not much on-time deliveries at that weight range*\n\nHere are several assumptions regarding the lateness in lighter products' weight:\n1. **Resource Imbalance**: Companies may prioritize shipments with weights over 4000 gr because they can generate more revenue. This can lead to resource imbalance, where products under 4000 gr are not adequately resourced.","metadata":{"id":"tHf19SYnjVuI"}},{"cell_type":"markdown","source":"### Business Recommendations\n\nThis section provides insights and recommendations based on the current dataset. For more accurate and specific follow-up actions, additional data may be required, depending on the type of factors that cause late deliveries.\n\n1. **Discount factor:** There is a tendency that the larger the discount given, the more likely the order is to be late. This could be due to several factors, such as high demand for products with large discounts or possible impacts on the delivery process. Interim recommendations that can be made:\n\n  - **Implementing a Discount Threshold Policy (to be simulated later):** Simulate the impact of setting a maximum discount threshold (10%) for orders. Test whether limiting discounts to this threshold can reduce late deliveries. This policy can help balance between attracting customers through discounts and ensuring on-time delivery.\n\n  - **Shipping System:** Consider optimizing the shipping system/routes for products with large discounts, so that items always arrive on time for products with large discounts.\n\n  - **Continuously Monitor:** Monitor late deliveries of products with large discounts specifically and strive to minimize them.\n\n  *Note: Limiting discounts may seem counterintuitive. However, here are some of the things we can gain from this policy:*\n\n    - Reduce Late Deliveries: As has been observed, higher discounts are associated with late deliveries. Limiting discounts can help ensure that the company meets its delivery commitments and maintains a better on-time delivery rate, which is essential for customer satisfaction.\n\n    - Improve Profit Margins: By limiting discounts to 10%, the company can maintain better profit margins on its products. Higher discounts can significantly reduce profits.\n\n2. **Product Importance Influence:** Products categorized as \"High\" are more likely to be late. This suggests that the \"High\" category may require special attention to meet delivery schedules. We need additional data on the characteristics of \"High\" importance products to analyze in more detail. However, there are some interim suggestions that can be made:\n\n  - **Dedicated Team Assignment:** Form a dedicated team for the delivery of \"high\" importance products. This team will have the resources and expertise necessary to ensure on-time delivery for \"high\" importance items.\n\n  - **Premium Shipping Services:** Use shipping companies with a good reputation and track record, especially for \"High\" importance products. This ensures on-time and fast delivery, even if there is a potential for late delivery.\n\n3. **Weight Factor:** It is seen that some weight ranges (mainly in the 1000-4000 gr range) of products tend to be late. This could be due to the complexity of the process of shipping several items with the mentioned weight ranges or the need for different shipping methods. We need more data on the characteristics of products that have weights in those ranges. Here are some interim recommendations that can be implemented:\n\n  - **Further Analysis:** Identify in more detail why products with the aforementioned weights are late. Are there any problems in the delivery process that need to be fixed? Consider adjusting the shipping or logistics process specifically for these products so that they can be shipped more efficiently and on time. Ensure that customers are given realistic delivery estimates for products with certain weights.\n\n  - **Product Packaging:** One of the reasons why some items with certain weights are prone to being late can be caused by the packaging of the item itself:\n\n    - **Packaging Strength:** Evaluate whether the packaging used for products in that range is strong enough to withstand the weight and handling during shipping. Weak packaging can cause product damage and order delays.\n\n    - **Seal Quality:** Make sure the sealing of these packages is secure. Insufficient sealing can cause items to shift during transportation, potentially leading to delays and damage.\n\n    - **Weight Label Accuracy:** Make sure the accuracy of the weight label on the package is correct. The wrong weight labeling process can result in the selection of an inappropriate transportation method, leading to late delivery (e.g., items with heavy weight labels are shipped using small modes of transportation, and vice versa).\n\nOverall, it is important for businesses to continuously monitor and improve logistics and delivery processes to ensure timely and reliable service to customers. These recommendations should be a starting point for addressing potential problems and optimizing on-time delivery rates.","metadata":{"id":"sXjWDFZeAKHa"}},{"cell_type":"markdown","source":"# Cleaning and Preprocessing\nBefore going into the modelling phase, we need to clean and preprocess our data to make the learning process more efficient and appropriate.\n\n## Data Cleaning\n\n### Missing Values Inspection","metadata":{"id":"l6NuZlAJmDnk"}},{"cell_type":"code","source":"df.isna().sum()","metadata":{"id":"j9ZG5qe8BKCe","outputId":"64aa6414-1c58-4e4d-b12f-4e4dea93b504","execution":{"iopub.status.busy":"2023-11-11T02:45:53.467522Z","iopub.execute_input":"2023-11-11T02:45:53.468891Z","iopub.status.idle":"2023-11-11T02:45:53.489330Z","shell.execute_reply.started":"2023-11-11T02:45:53.468853Z","shell.execute_reply":"2023-11-11T02:45:53.487909Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There is no empty data so we don't need to handle them\n\n","metadata":{"id":"IBzSV6HynDlK"}},{"cell_type":"markdown","source":"### Duplicates Inspection","metadata":{"id":"XQB0pAiIBDvB"}},{"cell_type":"code","source":"df.duplicated().sum()","metadata":{"id":"92yOzVASBLOe","outputId":"5273af45-8cad-4625-f3b2-3fbd3fc72712","execution":{"iopub.status.busy":"2023-11-11T02:45:53.490903Z","iopub.execute_input":"2023-11-11T02:45:53.491311Z","iopub.status.idle":"2023-11-11T02:45:53.514783Z","shell.execute_reply.started":"2023-11-11T02:45:53.491279Z","shell.execute_reply":"2023-11-11T02:45:53.513186Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There is no duplicated data so we don't need to handle them","metadata":{"id":"9IplAMeTnpH1"}},{"cell_type":"markdown","source":"### Outliers Inspection\nWe'll base our outlier detection using the most common approach: Boxplot's IQR","metadata":{"id":"c-qB0eN5BGS1"}},{"cell_type":"code","source":"# Box Plot\nplt.figure(figsize = (5, 5))\n\nfor i in range(0, len(nums)):\n  plt.subplot(2, 3, i+1)\n  sns.boxplot(y = df[nums[i]], color='darkblue', medianprops= dict(color='red'))\n\nplt.tight_layout()\nplt.show()","metadata":{"id":"q1wdaJ_RBJf5","outputId":"50e43f4f-a0ea-4930-b4a9-ce1b6cd34494","execution":{"iopub.status.busy":"2023-11-11T02:45:53.516803Z","iopub.execute_input":"2023-11-11T02:45:53.517536Z","iopub.status.idle":"2023-11-11T02:45:54.305359Z","shell.execute_reply.started":"2023-11-11T02:45:53.517502Z","shell.execute_reply":"2023-11-11T02:45:54.304042Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have several collective outliers in the `Discount_offered` feature but the outlier values are still appropriate (`Discount_offered` ranges between 0% to 100%). Hence, we'll not drop any of them because despite outliers, they are still valid observations.\n\nRegarding the `Prior_purchases`, we see several outliers but all of them still appropriate (6, 7, 8, or 10 prior purchases is still a valid number). Therefore, we'll not drop any of them because despite outliers, they're still valid observations.","metadata":{"id":"Ba_Yth0M55Q6"}},{"cell_type":"markdown","source":"### Class Imbalance Inspection","metadata":{"id":"nRYYw4-qBN0_"}},{"cell_type":"code","source":"# Class imbalance check in percentage (%)\nround(df['Delivery_status'].value_counts(normalize = True)*100, 2)","metadata":{"id":"QseM5g_BBTvg","outputId":"7581908f-51fe-4890-8527-da02c02ff8c5","execution":{"iopub.status.busy":"2023-11-11T02:45:54.307124Z","iopub.execute_input":"2023-11-11T02:45:54.309509Z","iopub.status.idle":"2023-11-11T02:45:54.323366Z","shell.execute_reply.started":"2023-11-11T02:45:54.309450Z","shell.execute_reply":"2023-11-11T02:45:54.322067Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The dataset is slightly imbalanced but no further action is needed.","metadata":{"id":"EkjNk7d9YWmu"}},{"cell_type":"markdown","source":"### Feature with Multicolinearity Removal\nAs we mentioned before in the EDA, there are several features with multicollinearity but we will retain the feature with the most correlation with our target `Delivery_status`","metadata":{"id":"Gcv7BBoRBUwX"}},{"cell_type":"code","source":"# Independent variables set\nX = df[nums]\n\n# VIF dataframe\nvif_data = pd.DataFrame()\nvif_data[\"feature\"] = X.columns\n\n# calculating VIF for each feature\nvif_data[\"VIF\"] = [variance_inflation_factor(X.values, i)\n                          for i in range(len(X.columns))]\n\nvif_data.sort_values('VIF', ascending = False)","metadata":{"id":"4c0bOR47KbVZ","outputId":"0f46695d-c669-43c6-e70b-747afc7b1b8b","execution":{"iopub.status.busy":"2023-11-11T02:45:54.325419Z","iopub.execute_input":"2023-11-11T02:45:54.325943Z","iopub.status.idle":"2023-11-11T02:45:54.386242Z","shell.execute_reply.started":"2023-11-11T02:45:54.325895Z","shell.execute_reply":"2023-11-11T02:45:54.384652Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As a reminder, features that could cause multicollinearity are those with VIF > 10, namely `Cost_of_the_Product` and `Customer_care_calls`.","metadata":{"id":"pgOFeGofY52x"}},{"cell_type":"code","source":"# Correlation Heatmap\nplt.figure(figsize=(6, 6))\nsns.heatmap(df.corr(numeric_only = True), cmap='cividis', annot=True, fmt='.2f')","metadata":{"id":"BS6qU4A4BXlY","outputId":"a218f50e-7343-42c3-e2f6-eb99ac220b61","execution":{"iopub.status.busy":"2023-11-11T02:45:54.388519Z","iopub.execute_input":"2023-11-11T02:45:54.390416Z","iopub.status.idle":"2023-11-11T02:45:54.858404Z","shell.execute_reply.started":"2023-11-11T02:45:54.390345Z","shell.execute_reply":"2023-11-11T02:45:54.857464Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Despite the high VIF value of `Cost_of_the_Product`, it has better correlation with the target. Hence, we'll remove `Customer_care_calls` instead with lower correlation with the target.","metadata":{"id":"A0hvEJfBLCFG"}},{"cell_type":"code","source":"df = df.drop(columns = 'Customer_care_calls')","metadata":{"id":"XF1HAkfjKgxx","execution":{"iopub.status.busy":"2023-11-11T02:45:54.859708Z","iopub.execute_input":"2023-11-11T02:45:54.860091Z","iopub.status.idle":"2023-11-11T02:45:54.869686Z","shell.execute_reply.started":"2023-11-11T02:45:54.860059Z","shell.execute_reply":"2023-11-11T02:45:54.868224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's check the VIF again\n\n","metadata":{"id":"krFD5bjJadQq"}},{"cell_type":"code","source":"# Independent variables set\nnums.remove('Customer_care_calls')\nX = df[nums]\n\n# VIF dataframe\nvif_data = pd.DataFrame()\nvif_data[\"feature\"] = X.columns\n\n# calculating VIF for each feature\nvif_data[\"VIF\"] = [variance_inflation_factor(X.values, i)\n                          for i in range(len(X.columns))]\n\nvif_data.sort_values('VIF', ascending = False)","metadata":{"id":"XIXeaiJ0Ks66","outputId":"93195f3d-f654-44be-b976-dc1038cfbaca","execution":{"iopub.status.busy":"2023-11-11T02:45:54.871371Z","iopub.execute_input":"2023-11-11T02:45:54.871971Z","iopub.status.idle":"2023-11-11T02:45:54.924630Z","shell.execute_reply.started":"2023-11-11T02:45:54.871936Z","shell.execute_reply":"2023-11-11T02:45:54.923018Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now there's no more feature that has VIF > 10 and cause further multicollinearity.","metadata":{"id":"iuC2ka1qbE82"}},{"cell_type":"markdown","source":"## Feature Creation / Extraction\n\n### Product Cost Class\nThis is a new feature that groups several ranges of values in the `Cost_of_the_Product` feature. We will base the grouping by its quantiles.","metadata":{}},{"cell_type":"code","source":"np.quantile(df['Cost_of_the_Product'], np.linspace(0, 1, 7))","metadata":{"id":"jGPXDtG1km2M","outputId":"04754a4a-63c3-4bfa-e124-db0d10c8ed30","execution":{"iopub.status.busy":"2023-11-11T02:45:54.926841Z","iopub.execute_input":"2023-11-11T02:45:54.927818Z","iopub.status.idle":"2023-11-11T02:45:54.943498Z","shell.execute_reply.started":"2023-11-11T02:45:54.927757Z","shell.execute_reply":"2023-11-11T02:45:54.941741Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the quantile result above, we'll divide the grouping range by:\n\n- Low = Orders with product that costs < \\$185\n- Medium = Orders with product that costs between \\$185 - \\$241\n- High = Orders with product that costs > \\$241","metadata":{"id":"cYzoYziAkq6-"}},{"cell_type":"code","source":"# Product cost class creation\ndef cost_classifier(Cost_of_the_Product):\n    if Cost_of_the_Product < 185:\n        return 'low'\n    elif Cost_of_the_Product < 241:\n        return 'medium'\n    else:\n        return 'high'\n\ndf['Cost_class'] = df['Cost_of_the_Product'].apply(cost_classifier)","metadata":{"id":"-rYCvFthRH8Z","execution":{"iopub.status.busy":"2023-11-11T02:45:54.946897Z","iopub.execute_input":"2023-11-11T02:45:54.948200Z","iopub.status.idle":"2023-11-11T02:45:54.971369Z","shell.execute_reply.started":"2023-11-11T02:45:54.948138Z","shell.execute_reply":"2023-11-11T02:45:54.969662Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's see if there are any differences in the delivery status rate between the cost classes.","metadata":{"id":"ed1HwDyYkz29"}},{"cell_type":"code","source":"plt.figure(figsize = (10, 5))\n\n# Frequency base\nfig, axes = plt.subplots(1, 2, figsize=(10, 5))\nplt.subplots_adjust(right = 1.2)\nsns.despine(right = True)\n\nsns.countplot(ax = axes[0], x = df['Cost_class'], hue = df['Delivery_status'].sort_values(), order = ['low', 'medium', 'high'])\n\naxes[0].bar_label(axes[0].containers[0], label_type = 'edge', fontweight = 'normal')\naxes[0].bar_label(axes[0].containers[1], label_type = 'edge', fontweight = 'normal')\n\naxes[0].set(ylabel = 'Frequency', xlabel = 'Cost of Product Class')\naxes[0].set_title('Distribution of Delivery Status by Cost of Product Class', y = 1.05)\naxes[0].legend(['On Time', 'Late'], loc = 0, ncol = 1, title = 'Delivery_status', borderaxespad = 0.8)\naxes[0].margins(0, 0.2)\n\n# Proportion base\npc = df.groupby(['Cost_class', 'Delivery_status'])['Delivery_status'].count().reset_index(name = 'Freq')\npc['Prop'] = round(pc['Freq'] / pc.groupby('Cost_class')['Freq'].transform(sum) * 100, 2)\npc['Cost_class'] = pd.Categorical(pc['Cost_class'], ['low', 'medium', 'high'])\npc.sort_values('Cost_class', inplace = True)\npc.pivot_table(index='Cost_class', columns='Delivery_status', values='Prop').plot(kind='bar', stacked=True, ax=axes[1])\n\naxes[1].bar_label(axes[1].containers[0], label_type='center', fontweight='light', fmt='%.0f%%')\naxes[1].bar_label(axes[1].containers[1], label_type='center', fontweight='light', fmt='%.0f%%')\n\naxes[1].set_title('Proportion of Delivery Status by Cost of Product Class', y = 1.05)\naxes[1].set(ylabel = 'Proportion (%)', xlabel = 'Cost of Product Class')\naxes[1].set_xticks(ticks = axes[1].get_xticks(), labels = axes[1].get_xticklabels(), rotation = 0)\n\naxes[1].get_legend().set_visible(False)\n\nplt.show()\n","metadata":{"id":"YfRkUUDqRShx","outputId":"99cf8a8c-2306-43c3-961f-1164f7ec8877","execution":{"iopub.status.busy":"2023-11-11T02:45:54.974817Z","iopub.execute_input":"2023-11-11T02:45:54.976177Z","iopub.status.idle":"2023-11-11T02:45:55.582180Z","shell.execute_reply.started":"2023-11-11T02:45:54.976110Z","shell.execute_reply":"2023-11-11T02:45:55.580714Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This feature could be a good predictor to `Delivery_status` because as we can see above, the low, medium, and high product cost class delivery rate varies, where most deliveries are late in low cost class, followed by medium, and lastly high cost class.","metadata":{"id":"dAwmWK5s8CVo"}},{"cell_type":"markdown","source":"### Prior Purchases Class\nThis feature is the transformed version of `Prior_purchases` where the value will be grouped to low, medium, and high. For the grouping base range, we'll check the unique values that it has first.","metadata":{"id":"V6QK3aUcBj4n"}},{"cell_type":"code","source":"df['Prior_purchases'].value_counts().sort_index()","metadata":{"id":"4_PbvERNbcAf","outputId":"c3ad5d82-1628-4ce9-c312-5116d49bfb80","execution":{"iopub.status.busy":"2023-11-11T02:45:55.583707Z","iopub.execute_input":"2023-11-11T02:45:55.584085Z","iopub.status.idle":"2023-11-11T02:45:55.596015Z","shell.execute_reply.started":"2023-11-11T02:45:55.584054Z","shell.execute_reply":"2023-11-11T02:45:55.594801Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can formulate the grouping as:\n\n- low = Prior purchases < 4\n- medium = Prior purchases from 4 to 6\n- high = Prior purchases more than 6","metadata":{"id":"Xs3JLCbRbdR6"}},{"cell_type":"code","source":"# Prior purchase class creation\ndef prior_purchase_classifier(Prior_purchases):\n    if Prior_purchases < 4:\n        return 'low'\n    elif Prior_purchases < 7:\n        return 'medium'\n    else:\n        return 'high'\n\ndf['Prior_purchase_class'] = df['Prior_purchases'].apply(prior_purchase_classifier)","metadata":{"id":"kGTGOM3CBlbq","execution":{"iopub.status.busy":"2023-11-11T02:45:55.597917Z","iopub.execute_input":"2023-11-11T02:45:55.598410Z","iopub.status.idle":"2023-11-11T02:45:55.611795Z","shell.execute_reply.started":"2023-11-11T02:45:55.598340Z","shell.execute_reply":"2023-11-11T02:45:55.610532Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's see if there are any differences in the delivery status rate between the prior purchases classes.","metadata":{"id":"uQ5l36hhlCCy"}},{"cell_type":"code","source":"plt.figure(figsize = (10, 5))\n\n# Frequency base\nfig, axes = plt.subplots(1, 2, figsize=(10, 5))\nplt.subplots_adjust(right = 1.2)\nsns.despine(right = True)\n\nsns.countplot(ax = axes[0], x = df['Prior_purchase_class'], hue = df['Delivery_status'].sort_values(), order = ['low', 'medium', 'high'])\n\naxes[0].bar_label(axes[0].containers[0], label_type = 'edge', fontweight = 'normal')\naxes[0].bar_label(axes[0].containers[1], label_type = 'edge', fontweight = 'normal')\n\naxes[0].set(ylabel = 'Frequency', xlabel = 'Prior Purchase Class')\naxes[0].set_title('Distribution of Delivery Status by Prior Purchase class', y = 1.05)\naxes[0].legend(['On Time', 'Late'], loc = 0, ncol = 1, title = 'Delivery_status', borderaxespad = 0.8)\n\n# Proportion base\nppc = df.groupby(['Prior_purchase_class', 'Delivery_status'])['Delivery_status'].count().reset_index(name = 'Freq')\nppc['Prop'] = round(ppc['Freq'] / ppc.groupby('Prior_purchase_class')['Freq'].transform(sum) * 100, 2)\nppc['Prior_purchase_class'] = pd.Categorical(ppc['Prior_purchase_class'], ['low', 'medium', 'high'])\nppc.sort_values('Prior_purchase_class', inplace = True)\nppc.pivot_table(index='Prior_purchase_class', columns='Delivery_status', values='Prop').plot(kind='bar', stacked=True, ax=axes[1])\n\naxes[1].bar_label(axes[1].containers[0], label_type='center', fontweight='light', fmt='%.0f%%')\naxes[1].bar_label(axes[1].containers[1], label_type='center', fontweight='light', fmt='%.0f%%')\n\naxes[1].set_title('Proportion of Delivery Status by Prior Purchase class', y = 1.05)\naxes[1].set(ylabel = 'Proportion (%)', xlabel = 'Prior Purchase Class')\naxes[1].set_xticks(ticks = axes[1].get_xticks(), labels = axes[1].get_xticklabels(), rotation = 0)\n\naxes[1].get_legend().set_visible(False)\n\nplt.show()","metadata":{"id":"HrEujMyDUFWc","outputId":"cc87ed58-d73e-4866-81cc-fb123c26a67f","execution":{"iopub.status.busy":"2023-11-11T02:45:55.613758Z","iopub.execute_input":"2023-11-11T02:45:55.614188Z","iopub.status.idle":"2023-11-11T02:45:56.217690Z","shell.execute_reply.started":"2023-11-11T02:45:55.614154Z","shell.execute_reply":"2023-11-11T02:45:56.216264Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Low and High prior purchases class is suffering more late deliveries than the medium prior purchase class. We can use this feature as a predictor in the model.","metadata":{"id":"R6hz9Bid8MDJ"}},{"cell_type":"markdown","source":"## Feature Encoding\n\n### Ordinal Encoding","metadata":{"id":"dl1uoQhICRev"}},{"cell_type":"code","source":"def custom_ordinal_encoding(data, column_name):\n    def ordinal_classifier(x):\n        if x == \"low\":\n            return float(0)\n        elif x == \"medium\":\n            return float(1)\n        elif x == \"high\":\n            return float(2)\n\n    data[column_name] = data[column_name].apply(ordinal_classifier)\n\ncustom_ordinal_encoding(df, \"Product_importance\")\ncustom_ordinal_encoding(df, \"Cost_class\")\ncustom_ordinal_encoding(df, \"Prior_purchase_class\")","metadata":{"id":"0V5V04nFibHw","execution":{"iopub.status.busy":"2023-11-11T02:45:56.219414Z","iopub.execute_input":"2023-11-11T02:45:56.219876Z","iopub.status.idle":"2023-11-11T02:45:56.250337Z","shell.execute_reply.started":"2023-11-11T02:45:56.219843Z","shell.execute_reply":"2023-11-11T02:45:56.248765Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have implemented encoding in the category data used by creating a custom function called `custom_ordinal_encoding`. This function accepts column names, dataset, and categories as its input parameters, which allows us to efficiently and consistently perform ordinal encoding on each column that requires transformation, increasing clarity and reducing the risk of errors in the code, as well as maintaining consistency in the mapping of category values \"low,\" \"medium,\" and \"high.\"","metadata":{"id":"Vg4xur0xe0Tm"}},{"cell_type":"markdown","source":"## Converting Target back to Integer\nWe need to convert the target back to integer type so it can be used by the model.","metadata":{"id":"NyD4p_W7BtBo"}},{"cell_type":"code","source":"df['Delivery_status'] = df['Delivery_status'].astype('int64')","metadata":{"id":"kd0Uj-qUB2Cq","execution":{"iopub.status.busy":"2023-11-11T02:45:56.251960Z","iopub.execute_input":"2023-11-11T02:45:56.252395Z","iopub.status.idle":"2023-11-11T02:45:56.260872Z","shell.execute_reply.started":"2023-11-11T02:45:56.252361Z","shell.execute_reply":"2023-11-11T02:45:56.259861Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Splitting\nHere are the variables that we will use in the model:\n\n1. Numeric\n  - Discount_offered\n  - Weight_in_gms\n2. Categorical: Ordinal\n  - Product_importances\n  - Cost_class\n  - Prior_Purchase_class","metadata":{"id":"j0RGPs09B4nM"}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX = df[['Discount_offered', 'Weight_in_gms', 'Product_importance', 'Cost_class', 'Prior_purchase_class']]\ny = df[['Delivery_status']]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 24)\nX_train.shape, X_test.shape, y_train.shape, y_test.shape","metadata":{"id":"sV60vF09cKJi","outputId":"b4b36e3b-b211-4c53-909e-9fe3bf7d5cb4","execution":{"iopub.status.busy":"2023-11-11T02:45:56.262348Z","iopub.execute_input":"2023-11-11T02:45:56.263472Z","iopub.status.idle":"2023-11-11T02:45:56.281544Z","shell.execute_reply.started":"2023-11-11T02:45:56.263435Z","shell.execute_reply":"2023-11-11T02:45:56.280233Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature Transformation (Normalizing the Skewed Distribution)\n`Discount_offered` have a skewed distribution problem. We need to normalize the distribution before stepping into modelling. First, we'll see the minimum value that `Discount_offered` have so we would know what are the available methods for normalization\n\n*Note: As this first section is only inspecting the initial distribution shape and skewness plus benchmarking all the available transformation methods, we'll use the main dataset. Further transformation will be made after benchmark on splitted data*","metadata":{"id":"-dHJ5aN0CZd9"}},{"cell_type":"code","source":"df['Discount_offered'].min()","metadata":{"id":"zTjwKxx0A95I","outputId":"87e742d5-6be8-4cbf-b8d5-734c5a0ab501","execution":{"iopub.status.busy":"2023-11-11T02:45:56.283239Z","iopub.execute_input":"2023-11-11T02:45:56.283625Z","iopub.status.idle":"2023-11-11T02:45:56.292032Z","shell.execute_reply.started":"2023-11-11T02:45:56.283594Z","shell.execute_reply":"2023-11-11T02:45:56.290741Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We know that it doesn't have 0 or negative values, so we will benchmark the following normalization methods:\n1. Square Root\n2. Cube Root\n3. Log\n4. Reciprocal\n5. Box Cox\n6. Yeo-Johnson\n\nLet's see the shape and skewness first","metadata":{"id":"22pY3Z_iX73f"}},{"cell_type":"code","source":"sns.histplot(x=df['Discount_offered'],kde = True, color='y')","metadata":{"id":"mm91wz33cVhN","outputId":"1015a830-3dea-4824-aeca-3464e5923d6c","execution":{"iopub.status.busy":"2023-11-11T02:45:56.294188Z","iopub.execute_input":"2023-11-11T02:45:56.294709Z","iopub.status.idle":"2023-11-11T02:45:56.944954Z","shell.execute_reply.started":"2023-11-11T02:45:56.294665Z","shell.execute_reply":"2023-11-11T02:45:56.943758Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"old_skew = df[['Discount_offered']].skew().to_frame().reset_index()\nold_skew.columns = ['feature','old_skew']\nold_skew","metadata":{"id":"TOIGBW_ncZG0","outputId":"54d3fbb4-0de8-4e27-e2a6-0c815c7fd76f","execution":{"iopub.status.busy":"2023-11-11T02:45:56.946440Z","iopub.execute_input":"2023-11-11T02:45:56.946818Z","iopub.status.idle":"2023-11-11T02:45:56.962780Z","shell.execute_reply.started":"2023-11-11T02:45:56.946786Z","shell.execute_reply":"2023-11-11T02:45:56.961519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now let's do the benchmark.","metadata":{"id":"Gu_2dfXZYC7h"}},{"cell_type":"code","source":"from sklearn.preprocessing import PowerTransformer\n\n\nli = old_skew['feature'].values.tolist()\na = old_skew.copy()\n\n# Common Methods\nsqrt_list = []\ncbrt_list = []\nlog_list = []\nsq_list = []\ncb_list = []\n\nfor i in li:\n  sqrt_list.append(round(np.sqrt(X_train[i]).skew(), 2))\n  cbrt_list.append(round(np.cbrt(X_train[i]).skew(), 2))\n  log_list.append(round(np.log(X_train[i]).skew(), 2))\n  sq_list.append(round(np.square(X_train[i]).skew(), 2))\n  cb_list.append(round(np.power(X_train[i], 3).skew(), 2))\n\na['sqrt'] = sqrt_list\na['cbrt'] = cbrt_list\na['log'] = log_list\na['sq'] = sq_list\na['cb'] = cb_list\n\n# PowerTransformer\nbx_list = []\nyj_list = []\n\nfor i in li:\n  if 0 in df[i].values:\n    bx_list.append(None)\n  else:\n    box_cox = PowerTransformer(method = 'box-cox')\n    res = box_cox.fit_transform(X_train[i].values.reshape(len(X_train), 1)).tolist()\n    res = [item for sublist in res for item in sublist]\n    bx_list.append(pd.Series(res).skew().round(2))\n\na['box-cox'] = bx_list\n\nfor i in li:\n    yj = PowerTransformer()\n    res = yj.fit_transform(X_train[i].values.reshape(len(X_train), 1)).tolist()\n    res = [item for sublist in res for item in sublist]\n    yj_list.append(pd.Series(res).skew().round(2))\n\na['yeo-johnson'] = yj_list\na","metadata":{"id":"88ZItrJpHAvj","outputId":"8c227f9b-b0d6-4f68-ea98-e9b885cde172","execution":{"iopub.status.busy":"2023-11-11T02:45:56.964671Z","iopub.execute_input":"2023-11-11T02:45:56.965210Z","iopub.status.idle":"2023-11-11T02:45:57.047527Z","shell.execute_reply.started":"2023-11-11T02:45:56.965165Z","shell.execute_reply":"2023-11-11T02:45:57.046316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Observation result:\n\n- **Box-cox** normalization technique is the most effective in reducing skewness.\n\nNow let's fit the transformer to the train data and apply to both train and test","metadata":{"id":"1_ukqaCPcqv7"}},{"cell_type":"code","source":"# Transform process\ndisc_off_train = X_train[['Discount_offered']]\ndisc_off_test = X_test[['Discount_offered']]\n\npt = PowerTransformer(method = \"box-cox\").set_output(transform=\"pandas\")\nboxcox = pt.fit(disc_off_train)\n\ndisc_off_train = boxcox.transform(disc_off_train)\ndisc_off_test = boxcox.transform(disc_off_test)\n\nX_train['Discount_offered'] = disc_off_train\nX_test['Discount_offered'] = disc_off_test","metadata":{"id":"RvM9Dwk_JMrf","execution":{"iopub.status.busy":"2023-11-11T02:45:57.049400Z","iopub.execute_input":"2023-11-11T02:45:57.049751Z","iopub.status.idle":"2023-11-11T02:45:57.075853Z","shell.execute_reply.started":"2023-11-11T02:45:57.049719Z","shell.execute_reply":"2023-11-11T02:45:57.074661Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# X_train Skewness after transform\n\nX_train['Discount_offered'].skew()","metadata":{"id":"lpw5sYvwK9-V","outputId":"cb07360c-2ba6-4e1c-ceb6-0dd4344b21c3","execution":{"iopub.status.busy":"2023-11-11T02:45:57.077740Z","iopub.execute_input":"2023-11-11T02:45:57.078140Z","iopub.status.idle":"2023-11-11T02:45:57.086151Z","shell.execute_reply.started":"2023-11-11T02:45:57.078107Z","shell.execute_reply":"2023-11-11T02:45:57.084992Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# X_test Skewness after transform\n\nX_test['Discount_offered'].skew()","metadata":{"id":"UB9VyFJKLS8I","outputId":"1820ec70-42e6-4d1b-9330-5592d3957823","execution":{"iopub.status.busy":"2023-11-11T02:45:57.087589Z","iopub.execute_input":"2023-11-11T02:45:57.087958Z","iopub.status.idle":"2023-11-11T02:45:57.101842Z","shell.execute_reply.started":"2023-11-11T02:45:57.087927Z","shell.execute_reply":"2023-11-11T02:45:57.100365Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature Scaling\n\nWe will use StandardScaler as the scaling method because many of the features' initial distribution is approximately normal","metadata":{"id":"exaFFnNCChwB"}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\n# X_train scaling\nscaler = StandardScaler().set_output(transform=\"pandas\")\nscaler.fit(X_train)\nX_train = scaler.transform(X_train)\n\n# X_test scaling\nX_test = scaler.transform(X_test)","metadata":{"id":"iOH-EbmUYMwt","execution":{"iopub.status.busy":"2023-11-11T02:45:57.104051Z","iopub.execute_input":"2023-11-11T02:45:57.104504Z","iopub.status.idle":"2023-11-11T02:45:57.120566Z","shell.execute_reply.started":"2023-11-11T02:45:57.104462Z","shell.execute_reply":"2023-11-11T02:45:57.119341Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Modelling\n\n## Initial Evaluation for Model Selection\nRecall is the primary evaluation metric for this project because we want to minimize false negatives. False negatives are costly because they represent late deliveries that are predicted to be on time. This can make customers unhappy and less trusting of our company.","metadata":{"id":"dfmqMT8lpoGF"}},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\nfrom sklearn.model_selection import cross_validate","metadata":{"id":"XTcgankcpteJ","execution":{"iopub.status.busy":"2023-11-11T02:45:57.122377Z","iopub.execute_input":"2023-11-11T02:45:57.123293Z","iopub.status.idle":"2023-11-11T02:45:57.131479Z","shell.execute_reply.started":"2023-11-11T02:45:57.123241Z","shell.execute_reply":"2023-11-11T02:45:57.129841Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Cross validation function for initial evaluation\ndef cross_validation(models):\n    model_name = []\n\n    # Evaluation empty lists\n    cv_accuracy_mean = []\n    training_accuracy = []\n\n    cv_precision_mean = []\n    training_precision = []\n\n    cv_recall_mean = []\n    training_recall = []\n\n    cv_f1_mean = []\n    training_f1 = []\n\n    cv_roc_auc_mean = []\n    training_roc_auc = []\n\n    for name, model in models:\n        model_name.append(name)\n\n        # Initializing evaluation lists\n        scoring = ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']\n\n        # Cross validate\n        cv_score = cross_validate(model, X_train, y_train, scoring = scoring, cv = 5, return_train_score = True)\n\n        cv_accuracy_mean.append(cv_score['test_accuracy'].mean())\n        training_accuracy.append(cv_score['train_accuracy'].mean())\n\n        cv_precision_mean.append(cv_score['test_precision'].mean())\n        training_precision.append(cv_score['train_precision'].mean())\n\n        cv_recall_mean.append(cv_score['test_recall'].mean())\n        training_recall.append(cv_score['train_recall'].mean())\n\n        cv_f1_mean.append(cv_score['test_f1'].mean())\n        training_f1.append(cv_score['train_f1'].mean())\n\n        cv_roc_auc_mean.append(cv_score['test_roc_auc'].mean())\n        training_roc_auc.append(cv_score['train_roc_auc'].mean())\n\n    return pd.DataFrame({\n        'Model': model_name,\n        'Training Accuracy': training_accuracy,\n        'Test Accuracy': cv_accuracy_mean,\n        'Training Precision': training_precision,\n        'Test Precision': cv_precision_mean,\n        'Training Recall': training_recall,\n        'Test Recall': cv_recall_mean,\n        'Training F1': training_f1,\n        'Test F1': cv_f1_mean,\n        'Training ROC-AUC': training_roc_auc,\n        'Test ROC-AUC': cv_roc_auc_mean,\n    })","metadata":{"id":"JHzK8SiFpuaw","execution":{"iopub.status.busy":"2023-11-11T02:45:57.133654Z","iopub.execute_input":"2023-11-11T02:45:57.134924Z","iopub.status.idle":"2023-11-11T02:45:57.148772Z","shell.execute_reply.started":"2023-11-11T02:45:57.134883Z","shell.execute_reply":"2023-11-11T02:45:57.147337Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluate the model code (using the function above)\nmodels = [\n    ['Logistic Regression', LogisticRegression()],\n    ['Decision Tree', DecisionTreeClassifier()],\n    ['Random Forest', RandomForestClassifier(random_state=42)],\n    ['KNN', KNeighborsClassifier()],\n    ['SVC', SVC(random_state=42)],\n    ['Ada Boost', AdaBoostClassifier(random_state=42)],\n    ['XGBoost', XGBClassifier(random_state=42)]\n]\n\ninitial_eval = cross_validation(models)\ninitial_eval","metadata":{"id":"YieRd4Gip0xu","outputId":"c2168177-addb-41ec-c40d-f36548e49fa4","execution":{"iopub.status.busy":"2023-11-11T02:45:57.150549Z","iopub.execute_input":"2023-11-11T02:45:57.150990Z","iopub.status.idle":"2023-11-11T02:46:36.185479Z","shell.execute_reply.started":"2023-11-11T02:45:57.150943Z","shell.execute_reply":"2023-11-11T02:46:36.184092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Based on recall as the primary criterion, we will select the following three algorithms for hyperparameter tuning:**\n\n1. Logistic regression: This algorithm had the highest recall score in the initial evaluation and the best overall fit.\n2. Random forest: This algorithm had a good recall score and a good ROC-AUC score.\n3. XGBoost: This algorithm had a slightly lower recall score than random forest, but a slightly better accuracy and AUC. It also overfits less than random forest.\n\nWe will exclude the following algorithms from hyperparameter tuning:\n\n1. Decision tree: This algorithm had a good recall score, but a worse ROC-AUC score.\n2. Support vector machines (SVMs): This algorithm had the lowest initial recall score of all algorithms, despite having the highest accuracy.\n3. AdaBoost: This algorithm had a low recall score, despite having the highest ROC-AUC score.\n4. k-nearest neighbors (kNN): This algorithm had a good recall score, but a lower AUC than the other algorithms selected for hyperparameter tuning.","metadata":{"id":"AKVKjJQzTi_I"}},{"cell_type":"markdown","source":"## Hyperparameter Tuning\nFirst, we're gonna tune the ROC-AUC because a higher AUC score in a model indicates that the model is better at distinguishing between positive and negative classes. Then, we're going to increase the recall using threshold tuning.","metadata":{"id":"e7lhv2iIp7Dm"}},{"cell_type":"code","source":"# Function for threshold tuning and naive train-test evaluation\ndef eval_classification_threshold(model_name, model, threshold = 0.5):\n    y_pred_proba = model.predict_proba(X_test)\n    y_pred_proba_train = model.predict_proba(X_train)\n    y_pred = (model.predict_proba(X_test)[:, 1] >= threshold)\n    y_pred_train = (model.predict_proba(X_train)[:, 1] >= threshold)\n\n    accuracy = accuracy_score(y_test, y_pred)\n    accuracy_train = accuracy_score(y_train, y_pred_train)\n    precision = precision_score(y_test, y_pred)\n    precision_train = precision_score(y_train, y_pred_train)\n    recall = recall_score(y_test, y_pred)\n    recall_train = recall_score(y_train, y_pred_train)\n    f1 = f1_score(y_test, y_pred)\n    f1_train = f1_score(y_train, y_pred_train)\n    roc_auc = roc_auc_score(y_test, y_pred_proba[:, 1])\n    roc_auc_train = roc_auc_score(y_train, y_pred_proba_train[:, 1])\n\n    return pd.DataFrame({\n        'Model' : [model_name],\n        'Accuracy (Train)' : [accuracy_train],\n        'Accuracy (Test)' : [accuracy],\n        'Precision (Train)' : [precision_train],\n        'Precision (Test)' : [precision],\n        'Recall (Train)' : [recall_train],\n        'Recall (Test)' : [recall],\n        'F1 (Train)' : [f1_train],\n        'F1 (Test)' : [f1],\n        'ROC-AUC (Train)' : [roc_auc_train],\n        'ROC-AUC (Test)' : [roc_auc]\n    })","metadata":{"id":"LS-P0Hv-1BNN","execution":{"iopub.status.busy":"2023-11-11T02:46:36.187343Z","iopub.execute_input":"2023-11-11T02:46:36.187704Z","iopub.status.idle":"2023-11-11T02:46:36.199802Z","shell.execute_reply.started":"2023-11-11T02:46:36.187661Z","shell.execute_reply":"2023-11-11T02:46:36.198520Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Logistic Regression\nFor logistic regression, we're going to tune the following parameters:\n1. `penalty`\n2. `C`\n3. `solver`","metadata":{"id":"7Q-Fukvsp_Ts"}},{"cell_type":"code","source":"# Logit ROC-AUC tuning\nhyperparameters = {\n    'penalty' : ['l1', 'l2', 'elastic_net'],\n    'C' : np.linspace(0.0001, 0.1, 1000),\n    'solver' : ['lbfgs', 'liblinear', 'newton-cg', 'newton-cholesky', 'sag', 'saga']\n}\n\nlr = LogisticRegression()\nrs = RandomizedSearchCV(lr, hyperparameters, scoring = 'roc_auc', cv = 5, random_state = 10, n_iter = 100)\nrs.fit(X_train, y_train)\n\nprint(f\"Best parameter: {rs.best_params_}\")\nprint(rs.score(X_train, y_train), rs.best_score_)","metadata":{"id":"xTm5Mq-VqEXv","outputId":"495053d5-213d-45f9-b172-79df5e38be72","execution":{"iopub.status.busy":"2023-11-11T02:46:36.202258Z","iopub.execute_input":"2023-11-11T02:46:36.202796Z","iopub.status.idle":"2023-11-11T02:46:45.667938Z","shell.execute_reply.started":"2023-11-11T02:46:36.202751Z","shell.execute_reply":"2023-11-11T02:46:45.666718Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For reference, here's the initial evaluation of the logistic regression model.","metadata":{"id":"PxMI0fu_oSuV"}},{"cell_type":"code","source":"# Initial evaluation of logistic regression\ninitial_eval.loc[0, :].to_frame().T","metadata":{"id":"0d2k0MSEqHD7","outputId":"385e9995-aba7-483a-ebcf-6013d69d5b2d","execution":{"iopub.status.busy":"2023-11-11T02:46:45.669738Z","iopub.execute_input":"2023-11-11T02:46:45.670805Z","iopub.status.idle":"2023-11-11T02:46:45.702950Z","shell.execute_reply.started":"2023-11-11T02:46:45.670754Z","shell.execute_reply":"2023-11-11T02:46:45.701236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"And here's how the model perform on the training set (cross validated) after using the best hyperparameters. The model show a good fit.","metadata":{"id":"B86rtuiyoWoR"}},{"cell_type":"code","source":"# Cross validation of hyperparameter tuned logistic regression\nlr_hyp_eval = cross_validation([[\"Logistic Regression (Tuned)\", rs.best_estimator_]])\nlr_hyp_eval","metadata":{"id":"uejSIAINqNji","outputId":"2c73e930-9a4c-4f4f-c9b5-941e275c78dc","execution":{"iopub.status.busy":"2023-11-11T02:46:45.705415Z","iopub.execute_input":"2023-11-11T02:46:45.706805Z","iopub.status.idle":"2023-11-11T02:46:46.749224Z","shell.execute_reply.started":"2023-11-11T02:46:45.706734Z","shell.execute_reply":"2023-11-11T02:46:46.747110Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, let's try to tune the threshold to improve the recall.","metadata":{"id":"Eq4AQRoz1hRE"}},{"cell_type":"code","source":"# Threshold tuning\nlr = LogisticRegression(solver = 'liblinear', penalty = 'l2', C = 0.016900000000000002)\nlr.fit(X_train, y_train)\n\nlr_final_score = eval_classification_threshold('Logistic Regression', lr, 0.48)\nlr_final_score","metadata":{"id":"reKSDgLA1h2C","outputId":"e21ad34c-9ae9-4a2b-bc80-da143d9c3647","execution":{"iopub.status.busy":"2023-11-11T02:46:46.752817Z","iopub.execute_input":"2023-11-11T02:46:46.754321Z","iopub.status.idle":"2023-11-11T02:46:46.959264Z","shell.execute_reply.started":"2023-11-11T02:46:46.754246Z","shell.execute_reply":"2023-11-11T02:46:46.958062Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We got an increase of around 2.5% in recall after hyperparameter tuning with the following parameters for logistic regression:\n1. solver = 'liblinear'\n2. penalty = 'l2'\n3. C = 0.016900000000000002\n4. threshold = 0.48","metadata":{"id":"lo30gPtVoZLA"}},{"cell_type":"markdown","source":"### Random Forest\nFor random forest, we're going to tune the following parameters:\n1. `n_estimators`\n2. `criterion`\n3. `max_depth`\n4. `min_samples_split`\n5. `min_samples_leaf`\n6. `max_features`","metadata":{"id":"Q4HkvUIgqnmj"}},{"cell_type":"code","source":"# ROC-AUC tuning\nhyperparameters = {\n    'n_estimators' : [int(x) for x in np.linspace(1, 6, 6)],\n    'criterion' : ['gini', 'entropy'],\n    'max_depth' : [int(x) for x in np.linspace(5, 10, 5)],\n    'min_samples_split' : [int(x) for x in np.linspace(1, 10, 10)],\n    'min_samples_leaf' : [int(x) for x in np.linspace(1, 10, 10)],\n    'max_features' : ['auto', 'sqrt']\n}\n\nrf = RandomForestClassifier(random_state = 42)\nrs = RandomizedSearchCV(rf, hyperparameters, scoring = 'roc_auc', cv = 5, random_state = 10, n_iter = 100)\nrs.fit(X_train, y_train)\n\nprint(f\"Best parameter: {rs.best_params_}\")\nprint(rs.score(X_train, y_train), rs.best_score_)","metadata":{"id":"wR6GKN7Zq08H","outputId":"c83293fc-c4b2-4e13-950a-363606e2a6df","execution":{"iopub.status.busy":"2023-11-11T02:46:46.961054Z","iopub.execute_input":"2023-11-11T02:46:46.962466Z","iopub.status.idle":"2023-11-11T02:47:02.466768Z","shell.execute_reply.started":"2023-11-11T02:46:46.962393Z","shell.execute_reply":"2023-11-11T02:47:02.465054Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For reference, here's the initial evaluation of the random forest model.","metadata":{"id":"sxB03BetogIS"}},{"cell_type":"code","source":"# Initial evaluation of random forest\ninitial_eval.loc[2, :].to_frame().T","metadata":{"id":"-lbm42N7q08I","outputId":"46596a96-8df7-46c1-a3e4-ee48ad8c2d79","execution":{"iopub.status.busy":"2023-11-11T02:47:02.468853Z","iopub.execute_input":"2023-11-11T02:47:02.470078Z","iopub.status.idle":"2023-11-11T02:47:02.490095Z","shell.execute_reply.started":"2023-11-11T02:47:02.470018Z","shell.execute_reply":"2023-11-11T02:47:02.488469Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"And here's how the model perform on the training set (cross validated) after using the best hyperparameters. The model shows a good fit.","metadata":{"id":"XWnMijlAohCn"}},{"cell_type":"code","source":"# Cross validation of hyperparameter tuned random forest\nrf_hyp_eval = cross_validation([[\"Random Forest (Tuned)\", rs.best_estimator_]])\nrf_hyp_eval","metadata":{"id":"sNrGmySkq08J","outputId":"209f0fc6-4239-42aa-ff04-40b08ab742b9","execution":{"iopub.status.busy":"2023-11-11T02:47:02.492767Z","iopub.execute_input":"2023-11-11T02:47:02.493303Z","iopub.status.idle":"2023-11-11T02:47:03.142994Z","shell.execute_reply.started":"2023-11-11T02:47:02.493262Z","shell.execute_reply":"2023-11-11T02:47:03.141656Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, let's try to tune the threshold to improve the recall.","metadata":{"id":"SLNisxq815lI"}},{"cell_type":"code","source":"rf = RandomForestClassifier(n_estimators = 4, min_samples_split = 7, min_samples_leaf = 6,\n                            max_depth = 6, criterion = 'gini', max_features = 'auto', random_state = 42)\nrf.fit(X_train, y_train)\n\nrf_final_score = eval_classification_threshold('Random Forest', rf, 0.419)\nrf_final_score","metadata":{"id":"wWsIqzQa16Qd","outputId":"6ac1e9f2-62e6-49a6-aa9b-12e70cf62263","execution":{"iopub.status.busy":"2023-11-11T02:47:03.147544Z","iopub.execute_input":"2023-11-11T02:47:03.147955Z","iopub.status.idle":"2023-11-11T02:47:03.297677Z","shell.execute_reply.started":"2023-11-11T02:47:03.147925Z","shell.execute_reply":"2023-11-11T02:47:03.296476Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We got an increase of around 14% in recall after hyperparameter tuning with the following parameters for random forest:\n\n1. n_estimators = 4\n2. min_samples_split = 7\n3. min_samples_leaf = 6\n4. max_depth = 6\n5. criterion = 'gini'\n6. max_features = 'auto'\n7. threshold = 0.419","metadata":{"id":"OgD5SuGhojts"}},{"cell_type":"markdown","source":"### XGBoost\nFor XGBoost, we're going to tune the following parameters:\n1. `max_depth`\n2. `min_child_weight`\n3. `gamma`\n4. `tree_method`","metadata":{"id":"aEqAAEZjrC3P"}},{"cell_type":"code","source":"# ROC-AUC tuning\nhyperparameters = {\n    'max_depth' : [int(x) for x in np.linspace(1, 5, 5)],\n    'min_child_weight' : [int(x) for x in np.linspace(1, 50, 50)],\n    'gamma' : [float(x) for x in np.linspace(1, 5, 5)],\n    'tree_method' : ['auto', 'exact', 'approx', 'hist']\n}\n\nxgb = XGBClassifier(random_state = 42)\nrs = RandomizedSearchCV(xgb, hyperparameters, scoring = 'roc_auc', cv = 5, random_state = 10, n_iter = 100)\nrs.fit(X_train, y_train)\n\nprint(f\"Best parameter: {rs.best_params_}\")\nprint(rs.score(X_train, y_train), rs.best_score_)","metadata":{"id":"IvdCtZFerH5r","outputId":"32483043-0bde-4e14-b129-90f611053e4a","execution":{"iopub.status.busy":"2023-11-11T02:47:03.299450Z","iopub.execute_input":"2023-11-11T02:47:03.299905Z","iopub.status.idle":"2023-11-11T02:48:04.803989Z","shell.execute_reply.started":"2023-11-11T02:47:03.299871Z","shell.execute_reply":"2023-11-11T02:48:04.802970Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For reference, here's the initial evaluation of the XGBoost model.","metadata":{"id":"UpFgWolr0IoP"}},{"cell_type":"code","source":"# Initial evaluation of xgboost\ninitial_eval.loc[6, :].to_frame().T","metadata":{"id":"hT0WnM-trH6F","outputId":"36df1d4f-dabc-4fa9-b738-5e3e87705765","execution":{"iopub.status.busy":"2023-11-11T02:48:04.805641Z","iopub.execute_input":"2023-11-11T02:48:04.809444Z","iopub.status.idle":"2023-11-11T02:48:04.828385Z","shell.execute_reply.started":"2023-11-11T02:48:04.809376Z","shell.execute_reply":"2023-11-11T02:48:04.826891Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"And here's how the model perform on the training set (cross validated) after using the best hyperparameters. The model shows a good fit.","metadata":{"id":"gKcQ9yH20KW3"}},{"cell_type":"code","source":"# Cross validation of hyperparameter tuned xgboost\nxgb_hyp_eval = cross_validation([['XGBoost (Tuned)', rs.best_estimator_]])\nxgb_hyp_eval","metadata":{"id":"k2i-wKDJrH6G","outputId":"d69d22e6-9b88-4057-aa7b-ebb3e3d8503d","execution":{"iopub.status.busy":"2023-11-11T02:48:04.830494Z","iopub.execute_input":"2023-11-11T02:48:04.831082Z","iopub.status.idle":"2023-11-11T02:48:06.007055Z","shell.execute_reply.started":"2023-11-11T02:48:04.831034Z","shell.execute_reply":"2023-11-11T02:48:06.005703Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, let's try to tune the threshold to improve the recall.","metadata":{"id":"yvg4mTmr2Vq_"}},{"cell_type":"code","source":"xgb = XGBClassifier(max_depth = 2, min_child_weight = 29, gamma = 5.0,\n                    tree_method = 'exact', random_state = 42)\nxgb.fit(X_train, y_train)\n\nxgb_final_score = eval_classification_threshold('XGBoost', xgb, 0.447)\nxgb_final_score","metadata":{"id":"IiNrqzVT2W6v","outputId":"704336a2-039b-4268-f15c-df69d7abac09","execution":{"iopub.status.busy":"2023-11-11T02:48:06.009087Z","iopub.execute_input":"2023-11-11T02:48:06.011237Z","iopub.status.idle":"2023-11-11T02:48:06.270154Z","shell.execute_reply.started":"2023-11-11T02:48:06.011181Z","shell.execute_reply":"2023-11-11T02:48:06.268562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We got an increase of around 23% in recall after hyperparameter tuning with the following parameters for XGBoost:\n1. max_depth = 2\n2. min_child_weight = 29\n3. gamma = 5.0\n4. tree_method = 'exact'\n5. threshold = 0.447","metadata":{"id":"djpu2uEe0FHz"}},{"cell_type":"markdown","source":"## Final Evaluation for Model Selection\n\n**Naive Train Test Evaluation**\n\nIn this evaluation, we're going to select the model with the best recall score but not too overoptimistic and still has a decent accuracy and AUC score.","metadata":{"id":"erZ4J8PMrGkp"}},{"cell_type":"code","source":"# Train-test evaluation\npd.concat([lr_final_score, rf_final_score, xgb_final_score]).round(3).sort_values('Recall (Test)', ascending = False).reset_index(drop = True)","metadata":{"id":"wcFCM4MCrSXf","outputId":"292ead7b-0f36-4191-9058-35b77b6ced71","execution":{"iopub.status.busy":"2023-11-11T02:48:06.272211Z","iopub.execute_input":"2023-11-11T02:48:06.272743Z","iopub.status.idle":"2023-11-11T02:48:06.300399Z","shell.execute_reply.started":"2023-11-11T02:48:06.272698Z","shell.execute_reply":"2023-11-11T02:48:06.298504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Cross Validation Evaluation**\n\nThis evaluation aims to see if the model is indeed a good fit (good generalization). It seems that all of the model is indeed a good fit.\n\n*Note: We may see that the recall score has dropped significantly. This is because we didn't apply any threshold tuning to them and we just want to see the 'generalizability' of the model itself*","metadata":{"id":"UXB8te-ara2D"}},{"cell_type":"code","source":"# Cross validation result of tuned logistic regression, random forest, and xgboost\npd.concat([lr_hyp_eval, rf_hyp_eval, xgb_hyp_eval]).round(3).sort_values('Test Recall', ascending = False).reset_index(drop=True)","metadata":{"id":"iSdGELOirctd","outputId":"29ad1ee5-6c85-4656-c3cd-521cd795c74e","execution":{"iopub.status.busy":"2023-11-11T02:48:06.302810Z","iopub.execute_input":"2023-11-11T02:48:06.303378Z","iopub.status.idle":"2023-11-11T02:48:06.328095Z","shell.execute_reply.started":"2023-11-11T02:48:06.303329Z","shell.execute_reply":"2023-11-11T02:48:06.326798Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From both evaluations, we can see that Random Forest is the best model because:\n1. Has good recall score and not overoptimistic\n2. Has higher accuracy and AUC than other models","metadata":{"id":"shgACsiQqyKV"}},{"cell_type":"markdown","source":"**Confusion Matrix Evaluation**\n\nHere, we're going to check the distribution of True Negative, True Positive, False Positive, and False Negative produced from each final model.","metadata":{"id":"c0bKocO9rguw"}},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import ConfusionMatrixDisplay","metadata":{"id":"jvdhk8CQrjgW","execution":{"iopub.status.busy":"2023-11-11T02:48:06.330338Z","iopub.execute_input":"2023-11-11T02:48:06.330805Z","iopub.status.idle":"2023-11-11T02:48:06.338024Z","shell.execute_reply.started":"2023-11-11T02:48:06.330762Z","shell.execute_reply":"2023-11-11T02:48:06.336611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Logistic Regression\nlr = LogisticRegression(solver = 'liblinear', penalty = 'l2', C = 0.016900000000000002)\nlr.fit(X_train, y_train)\ny_pred = (lr.predict_proba(X_test)[:, 1] >= 0.48)\n\nfig, ax = plt.subplots(1, 3, figsize = (10, 10))\nConfusionMatrixDisplay(confusion_matrix(y_test, y_pred, labels = lr.classes_), display_labels = lr.classes_).plot(ax = ax[0],\n                                                                                                                  colorbar = False)\nax[0].set_title('Confusion Matrix of Logit')\nax[0].set_xlabel('')\n\n# Random Forest\nrf = RandomForestClassifier(n_estimators = 4, min_samples_split = 7, min_samples_leaf = 6,\n                            max_depth = 6, criterion = 'gini', max_features = 'auto', random_state = 42)\nrf.fit(X_train, y_train)\ny_pred = (rf.predict_proba(X_test)[:, 1] >= 0.419)\n\nConfusionMatrixDisplay(confusion_matrix(y_test, y_pred, labels = rf.classes_), display_labels = rf.classes_).plot(ax = ax[1],\n                                                                                                                  colorbar = False)\nax[1].set_title('Confusion Matrix of RF')\nax[1].set_ylabel('')\n\n# XGBoost\nxgb = XGBClassifier(max_depth = 2, min_child_weight = 29, gamma = 5.0, tree_method = 'exact', random_state = 42)\nxgb.fit(X_train, y_train)\ny_pred = (xgb.predict_proba(X_test)[:, 1] >= 0.447)\n\nConfusionMatrixDisplay(confusion_matrix(y_test, y_pred, labels = xgb.classes_), display_labels = xgb.classes_).plot(ax = ax[2],\n                                                                                                                    colorbar = False)\nax[2].set_title('Confusion Matrix of XGB')\nax[2].set_ylabel('')\nax[2].set_xlabel('')\n\nplt.tight_layout()","metadata":{"id":"bQRS5VVVmUXf","outputId":"b0e3591f-d162-4961-fcf7-d8c598125bca","execution":{"iopub.status.busy":"2023-11-11T02:48:06.340238Z","iopub.execute_input":"2023-11-11T02:48:06.340774Z","iopub.status.idle":"2023-11-11T02:48:07.214702Z","shell.execute_reply.started":"2023-11-11T02:48:06.340732Z","shell.execute_reply":"2023-11-11T02:48:07.213451Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The confusion matrix shows that random forest predicts late deliveries more accurately, but at the cost of increased false positives (on-time deliveries that are predicted late) but not as much as XGBoost. This is not a major problem, because the delivery will still be on time regardless of the late prediction. The core problem is late deliveries that are predicted on time, which can make customers unhappy and less trusting of our company. Random forest does a decent job of avoiding this problem than Logistic Regression although XGBoost does this better but it produces too many False Positives.","metadata":{"id":"_faQHnKvRkyY"}},{"cell_type":"markdown","source":"## Feature Importance","metadata":{"id":"Qv3-WGIgrnRT"}},{"cell_type":"code","source":"# Creating the feature importance dataframe\nfeat_imp = pd.DataFrame(rf.feature_importances_.tolist(), X.columns.tolist())\\\n           .reset_index(names = 'Feature').rename(columns = {0 : 'Importance'}).sort_values('Importance', ascending = False)\n\n# Plotting feature importance on a horizontal bar chart\nfig, ax = plt.subplots()\nsns.barplot(ax = ax, data = feat_imp, y = 'Feature', x = 'Importance', palette = 'Blues_r')\nax.bar_label(ax.containers[0], fmt = '%.2f', padding = 2)\n\nsns.despine(ax = ax, right = True)\nax.set_title('Random Forest Feature Importance')\nplt.show()","metadata":{"id":"CAkw32Z1rqRi","outputId":"d478767e-b55d-44b8-ee69-fb922a0e0ebf","execution":{"iopub.status.busy":"2023-11-11T02:48:07.216482Z","iopub.execute_input":"2023-11-11T02:48:07.217932Z","iopub.status.idle":"2023-11-11T02:48:07.543976Z","shell.execute_reply.started":"2023-11-11T02:48:07.217883Z","shell.execute_reply":"2023-11-11T02:48:07.542472Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Discount is the most important factor associated with delivery status, followed by weight. Prior purchase, cost of product, and product importance have a minor association with delivery status.\n\nBut we don't know the direction of these features towards the delivery status. For that, we're gonna use SHAP values.","metadata":{"id":"b6iCXD5qrvA7"}},{"cell_type":"code","source":"rf = RandomForestClassifier(n_estimators = 4, min_samples_split = 7, min_samples_leaf = 6,\n                            max_depth = 6, criterion = 'gini', max_features = 'auto', random_state = 42)\n\nrf.fit(X_train, y_train)\nexplainer = shap.Explainer(rf)\nshap_values = explainer.shap_values(X_test)\n\nshap.summary_plot(shap_values[1], X_test)","metadata":{"id":"4fHZaPlFrx8r","outputId":"e7fd07ab-e376-408e-8781-736fe47fbf05","execution":{"iopub.status.busy":"2023-11-11T02:48:07.546386Z","iopub.execute_input":"2023-11-11T02:48:07.546743Z","iopub.status.idle":"2023-11-11T02:48:08.455485Z","shell.execute_reply.started":"2023-11-11T02:48:07.546713Z","shell.execute_reply":"2023-11-11T02:48:08.454115Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Remember that a positive output (True / 1) in our dataset indicates a late delivery. Based on the summary plot above, we can infer the following:\n\n1. Higher discounts are more likely to be associated with late deliveries. This means that the more discount an order has, the higher the probability that it will be late.\n2. Lower product weights are more likely to be associated with late deliveries. This means that the lighter the weight of a product, the higher the probability that it will be late.\n3. Interestingly, people with more prior purchases are slightly less likely to experience late deliveries.\n4. Lower cost of products tend to be associated with late deliveries.\n5. Higher product importance tend to be associated with late deliveries.\n\nBased on these insights. We can take the following interim actions before any further analysis:\n1. **Implementing a Discount Threshold Policy (to be simulated)**\n\n    We propose to simulate the impact of setting a maximum discount threshold for orders. This policy could help balance attracting customers through discounts and ensuring on-time delivery. We will assess whether setting a discount threshold of 10% in this dataset will lead to an increase in on-time delivery rates.\n    \n    While this may increase on-time delivery rates, we need to investigate further why higher discounts lead to late deliveries. For this analysis, we would need more relevant data, such as detailed warehouse and logistics data. The dataset in this project is insufficient to identify the root causes of lateness at higher discount rates.\n\n    There is a risk that some customers may lose interest if we limit discounts. We need to clarify that this limit is temporary while we investigate the causes of late deliveries at higher discount rates and develop solutions. This will make us more trustworthy by demonstrating our transparency.\n\n    Because we are reducing discounts, we need to offer something else to customers in their place. This could be loyalty rewards, points, or early access to new products. This way, customers can still maintain their interest, albeit in a different form than discounts.\n\n2. **Bundled Products Shipment (to be simulated)**\n\n    One way to reduce the likelihood of late deliveries for lighter products is to bundle them together into heavier packages. For example, we could bundle several small toys together based on their region or address and count them as a single product in the logistics process. Once the bundles arrive at the local shipment warehouse, they can be unpacked.\n\n    This process would not be visible to customers. If we have a mobile app that tracks orders, each customer's order would still appear as a single order, not mixed with other customers' orders. This is because the bundling would only occur during the logistics process until the bundles reach the local shipment warehouse.\n\n    However, we still need to investigate the root causes of why lighter products are more likely to be late. For this, we would need the same dataset as before, including warehouse and logistics data. The dataset in this project is not sufficient to identify the causes of late deliveries for lighter products.\n\nThere is one last prescriptive recommendations for those customers who will experience late deliveries:\n\n- **Inform customers of the delay and extend the estimated delivery date.**\n\n    The first thing to do is to contact/inform the customer that the delivery will be delayed so that they will not be surprised if the product hasn't been delivered by the estimated delivery date. Then, we can add *x* days to the estimated delivery date to extend the range.\n\n- **Offer incentives to customers who experience late deliveries**\n    \n    Because higher discount in this dataset is associated with lateness, we can offer other alternative incentives to them, such as loyalty points, free delivery on the next order, exclusive access to new product/market section, and many more. Consider offering customers the option to choose the incentive that they prefer. This will give customers more control over the situation and make them more likely to be satisfied with the outcome.","metadata":{"id":"G3UCwasrr1vJ"}},{"cell_type":"markdown","source":"# Business Recommendation: Discount Threshold & Weight Simulation\nIn this section, we're going to simulate what will happen if we limit the discount and increase the weight (bundle simulation) of the shipment.","metadata":{"id":"H0KCEIaK3Smm"}},{"cell_type":"code","source":"# Creating the simulation dataset from the original dataset\nX_sim = X.copy()\n\n# Setting the random seed for discount simulation\nnp.random.seed(42)\n\n# Simulating the discount threshold of 4%\nX_sim['Discount_offered'] = X_sim['Discount_offered'].apply(lambda x : np.random.randint(1, 4))\nX_sim['Weight_in_gms'] = X_sim['Weight_in_gms'].apply(lambda x : np.random.randint(5000, 6000) if x < 4500 else x)\n#X_sim['pp_class_enc'] = X_sim['pp_class_enc'].apply(lambda x : float(1) if x < 1.0 else x)\n\n# Transforming the discount feature again\nX_sim['Discount_offered'] = boxcox.transform(np.array(X_sim['Discount_offered']).reshape(-1, 1))\n\n# Standardizing the rest of the features\nX_sim = scaler.transform(X_sim)\n\n# Predicting on the new simulated dataset\ny_pred_sim = (rf.predict_proba(X_sim)[:, 1] >= 0.419)\n\n# Seeing the delivery status distribution\npd.Series(y_pred_sim).value_counts(normalize = True)","metadata":{"id":"IuCtGOWM3ftl","outputId":"8a6813fd-285b-464b-edbf-07a10801cc42","execution":{"iopub.status.busy":"2023-11-11T02:48:08.457086Z","iopub.execute_input":"2023-11-11T02:48:08.457431Z","iopub.status.idle":"2023-11-11T02:48:08.621236Z","shell.execute_reply.started":"2023-11-11T02:48:08.457400Z","shell.execute_reply":"2023-11-11T02:48:08.619994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Getting the original dataset delivery status distribution\noriginal_perc = pd.Series(y_pred_sim).value_counts(normalize = True)[[0]].to_frame().reset_index()\noriginal_perc.columns = ['Dataset', 'On-time percentage']\noriginal_perc.iloc[0, 0] = 'Simulated'\n\n# Getting the simulated dataset delivery status distribution\nsim_perc = df['Delivery_status'].value_counts(normalize = True)[[0]].to_frame().reset_index()\nsim_perc.columns = ['Dataset', 'On-time percentage']\nsim_perc.iloc[0, 0] = 'Original'\n\n# Concatenating both result from original and simulated dataset\npercent = pd.concat([original_perc, sim_perc], ignore_index = True)\npercent\n\n# Creating the plot\nfig, ax = plt.subplots()\nsns.barplot(y = 'On-time percentage', x = 'Dataset', data = percent, order = ['Original', 'Simulated'], palette = ['#565656', 'darkblue'])\nsns.despine(right = True)\n\n# Setting bar label\nbarlab = ax.bar_label(ax.containers[0], c = 'black', fmt = '%.2f', label_type = 'center')\nbarlab[0].set(y = 60, c = 'darkgrey')\nbarlab[1].set(y = 110, c = '#C7A300', weight = 'bold')\n\n# Setting spine, ticks, and label colors\nax.spines['bottom'].set_color('#565656')\nax.spines['left'].set_color('#565656')\nax.tick_params(axis='both', colors='#565656')\nax.set_xlabel('Delivery status', size = 9, c = '#565656')\nax.set_ylabel(ax.get_ylabel() + ' (%)', size = 9, c = '#565656')\nax.set_xticklabels(ax.get_xticklabels(), c = '#565656')\n\nxtick = ax.get_xticklabels()\nxtick[1].set(color = 'darkblue')\n\n# Annotating the increase in on-time delivery rate\nax.annotate(xytext = (0.55, 0.4), xy = (0.55, 0.58), text = '', arrowprops = dict(arrowstyle = '-', color = '#C7A300'))\nplt.text(s = '18% Increase of\\non-time delivery rate', x = 0.51, y = 0.48, color = 'darkblue', ha = 'right')\n\n# Inserting title and subtitle\nplt.title('Original vs. Simulated On-Time Delivery Rates', weight = 'bold', c = 'darkblue', x = 0.41, y = 1.15, size = 14)\nplt.text(s = 'Discount and weight are the main factors in increasing\\nthe on-time delivery rate', c = '#404040', x = -0.69, y = 0.65)\n\nplt.show()","metadata":{"id":"enCcjvVa3qTR","outputId":"f6f70723-7a0d-495e-81bb-f9555ac366ae","execution":{"iopub.status.busy":"2023-11-11T02:48:08.622703Z","iopub.execute_input":"2023-11-11T02:48:08.623063Z","iopub.status.idle":"2023-11-11T02:48:08.917453Z","shell.execute_reply.started":"2023-11-11T02:48:08.623034Z","shell.execute_reply":"2023-11-11T02:48:08.916226Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The following are the interim solution/action that we could take based on the simulation:\n1. Surprisingly, to better increase our on-time delivery rate we need to put a 4% threshold instead of 10%. After carefully reviewing the visualization of Weight vs. Delivery Status relationship, we can see that there are even some late deliveries in the 4% to 10% range.\n2. We need to 'bundle' products with weight less than 5000 gr to be 5000 gr in the logistic process to increase the on-time delivery rate. Therefore, there need to be a additional shipment identifier for each bundle of the shipment later on.\n3. There is one more catch: The more prior purchases that a customer has, the more likely that their delivery will be on-time. **This should be further analyzed later on because we're neglecting these new customers' delivery commitment!**","metadata":{"id":"cA-ya24h3Z1n"}}]}